<!DOCTYPE html>
<html lang="en">
<head>
        <title>Edwin Chen's Blog</title>
        <meta charset="utf-8" />
	      <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie.css"/>
                <script src="/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie6.css"/><![endif]-->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js" type="text/javascript"></script>


</head>

<body id="index" class="home">
	
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/2011/10/24/winning-the-netflix-prize-a-summary/">Winning the Netflix Prize: A Summary</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/edwin-chen.html">Edwin Chen</a>
        </li>
        <li class="published" title="2011-10-24T04:15:00">
          on&nbsp;Mon 24 October 2011
        </li>

	</ul>

</div><!-- /.post-info -->
  <div class="entry-content"><p>How was the <a href="http://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize</a> won? I went through a lot of the Netflix Prize papers a couple years ago, so I&#8217;ll try to give an overview of the techniques that went into the winning solution here.</p>

  <h1>Normalization of Global Effects</h1>

  <p>Suppose Alice rates Inception 4 stars. We can think of this rating as composed of several parts:</p>

  <ul>
  <li>A <strong>baseline rating</strong> (e.g., maybe the mean over all user-movie ratings is 3.1 stars).</li>
  <li>An <strong>Alice-specific effect</strong> (e.g., maybe Alice tends to rate movies lower than the average user, so her ratings are -0.5 stars lower than we normally expect).</li>
  <li>An <strong>Inception-specific effect</strong> (e.g., Inception is a pretty awesome movie, so its ratings are 0.7 stars higher than we normally expect).</li>
  <li>A less predictable effect based on the <strong>specific interaction</strong> between Alice and Inception that accounts for the remainder of the stars (e.g., Alice really liked Inception because of its particular combination of Leonardo DiCaprio and neuroscience, so this rating gets an additional 0.7 stars).</li>
  </ul>


  <p>In other words, we&#8217;ve decomposed the 4-star rating into:
  4 = [3.1 (the baseline rating) - 0.5 (the Alice effect) + 0.7 (the Inception effect)] + 0.7 (the specific interaction)</p>

  <p>So instead of having our models predict the 4-star rating itself, we could first try to remove the effect of the baseline predictors (the first three components) and have them predict the specific 0.7 stars. (I guess you can also think of this as a simple kind of boosting.)</p>

  <p>More generally, additional baseline predictors include:</p>

  <ul>
  <li>A factor that allows Alice&#8217;s rating to (linearly) depend on the (square root of the) <strong>number of days since her first rating</strong>. (For example, have you ever noticed that you become a harsher critic over time?)</li>
  <li>A factor that allows Alice&#8217;s rating to depend on the <strong>number of days since the movie&#8217;s first rating by anyone</strong>. (If you&#8217;re one of the first people to watch it, maybe it&#8217;s because you&#8217;re a huge fan and really excited to see it on DVD, so you&#8217;ll tend to rate it higher.)</li>
  <li>A factor that allows Alice&#8217;s rating to depend on the <strong>number of people who have rated Inception</strong>. (Maybe Alice is a hipster who hates being part of the crowd.)</li>
  <li>A factor that allows Alice&#8217;s rating to <strong>depend on the movie&#8217;s overall rating</strong>.</li>
  <li>(Plus a bunch of others.)</li>
  </ul>


  <p>And, in fact, modeling these biases turned out to be fairly important: in their paper describing their final solution to the Netflix Prize, Bell and Koren write that</p>

  <blockquote><p>Of the numerous new algorithmic contributions, I would like to highlight one &#8211; those humble baseline predictors (or biases), which capture main effects in the data. While the literature mostly concentrates on the more sophisticated algorithmic aspects, we have learned that an accurate treatment of main effects is probably at least as signficant as coming up with modeling breakthroughs.</p></blockquote>


  <p>(For a perhaps more concrete example of why removing these biases is useful, suppose you know that Bob likes the same kinds of movies that Alice does. To predict Bob&#8217;s rating of Inception, instead of simply predicting the same 4 stars that Alice rated, if we know that Bob tends to rate movies 0.3 stars higher than average, then we could first remove Alice&#8217;s bias and then add in Bob&#8217;s: 4 + 0.5 + 0.3 = 4.8.)</p>

  <h1>Neighborhood Models</h1>

  <p>Let&#8217;s now look at some slightly more sophisticated models. As alluded to in the section above, one of the standard approaches to collaborative filtering is to use neighborhood models.</p>

  <p>Briefly, a neighborhood model works as follows. To predict Alice&#8217;s rating of Titanic, you could do two things:</p>

  <ul>
  <li><strong>Item-item approach</strong>: find a set of items similar to Titanic that Alice has also rated, and take the (weighted) mean of Alice&#8217;s ratings on them.</li>
  <li><strong>User-user approach</strong>: find a set of users similar to Alice who rated Titanic, and again take the mean of their ratings of Titanic.</li>
  </ul>


  <p>(See also my post on <a href="http://blog.echen.me/2011/02/15/an-overview-of-item-to-item-collaborative-filtering-with-amazons-recommendation-system/">item-to-item collaborative filtering on Amazon</a>.)</p>

  <p>The main questions, then, are (let&#8217;s stick to the item-item approach for simplicity):</p>

  <ul>
  <li>How do we find the set of similar items?</li>
  <li>How do we weight these items when taking their mean?</li>
  </ul>


  <p>The standard approach is to take some similarity metric (e.g., correlation or a Jaccard index) to define similarities between pairs of movies, take the K most similar movies under this metric (where K is perhaps chosen via cross-validation), and then use the same similarity metric when computing the weighted mean.</p>

  <p>This has a couple problems:</p>

  <ul>
  <li><strong>Neighbors aren&#8217;t independent</strong>, so using a standard similarity metric to define a weighted mean overcounts information. For example, suppose you ask five friends where you should eat tonight. Three of them went to Mexico last week and are sick of burritos, so they strongly recommend against a taqueria. Thus, your friends&#8217; recommendations have a stronger bias than what you&#8217;d get if you asked five friends who didn&#8217;t know each other at all. (Compare with the situation where all three Lord of the Rings Movies are neighbors of Harry Potter.)</li>
  <li>Different movies should perhaps be using <strong>different numbers of neighbors</strong>. Some movies may be predicted well by only one neighbor (e.g., Harry Potter 2 could be predicted well by Harry Potter 1 alone), some movies may require more, and some movies may have no good neighbors (so you should ignore your neighborhood algorithms entirely and let your other ratings models stand on their own).</li>
  </ul>


  <p>So another approach is the following:</p>

  <ul>
  <li>You can still use a similarity metric like correlation or cosine similarity to choose the set of similar items.</li>
  <li>But instead of using the similarity metric to define the interpolation weights in the mean calculations, you essentially perform a (sparse) <strong>linear regression to find the weights</strong> that minimize the squared error between an item&#8217;s rating and a linear combination of the ratings of its neighbors. Note that these weights are no longer constrained, so that if all neighbors are weak, then their weights will be close to zero and the neighborhood model will have a low effect.</li>
  </ul>


  <p>(A slightly more complicated user-user approach, similar to this item-item neighborhood approach, is also useful.)</p>

  <h1>Implicit Data</h1>

  <p>Adding on to the neighborhood approach, we can also let <strong>implicit data influence our predictions</strong>. The mere fact that a user rated lots of science fiction movies but no westerns, suggests that the user likes science fiction better than cowboys. So using a similar framework as in the neighborhood ratings model, we can learn for Inception a set of <strong>offset weights</strong> associated to Inception&#8217;s movie neighbors.</p>

  <p>Whenever we want to predict how Bob rates Inception, we look at whether Bob rated each of Inception&#8217;s neighbors. If he did, we add in the corresponding offset; if not, then we add nothing (and, thus, Bob&#8217;s rating is implicitly penalized by the missing weight).</p>

  <h1>Matrix Factorization</h1>

  <p>Complementing the neighborhood approach to collaborative filtering is the matrix factorization approach. Whereas the neighborhood approach takes a very local approach to ratings (if you liked Harry Potter 1, then you&#8217;ll like Harry Potter 2!), the factorization approach takes a more global view (we know that you like fantasy movies and that Harry Potter has a strong fantasy element, so we think that you&#8217;ll like Harry Potter) that <strong>decomposes users and movies into a set of latent factors</strong> (which we can think of as categories like &#8220;fantasy&#8221; or &#8220;violence&#8221;).</p>

  <p>In fact, matrix factorization methods were probably the most important class of techniques for winning the Netflix Prize. In their 2008 Progress Prize paper, Bell and Koren write</p>

  <blockquote><p>It seems that models based on matrix-factorization were found to be most accurate (and thus popular), as evident by recent publications and discussions on the Netflix Prize forum. We definitely agree to that, and would like to add that those matrix-factorization models also offer the important flexibility needed for modeling temporal effects and the binary view. Nonetheless, neighborhood models, which have been dominating most of the collaborative filtering literature, are still expected to be popular due to their practical characteristics - being able to handle new users/ratings without re-training and offering direct explanations to the recommendations.</p></blockquote>


  <p>The typical way to perform matrix factorizations is to perform a <strong>singular value decomposition</strong> on the (sparse) ratings matrix (using stochastic gradient descent and regularizing the weights of the factors, possibly constraining the weights to be positive to get a type of non-negative matrix factorization). (Note that this &#8220;SVD&#8221; is a little different from the standard SVD learned in linear algebra, since not every user has rated every movie and so the ratings matrix contains many missing elements that we don&#8217;t want to simply treat as 0.)</p>

  <p>Some SVD-inspired methods used in the Netflix Prize include:</p>

  <ul>
  <li><strong>Standard SVD</strong>: Once you&#8217;ve represented users and movies as factor vectors, you can dot product Alice&#8217;s vector with Inception&#8217;s vector to get Alice&#8217;s predicted rating of Inception.</li>
  <li><strong>Asymmetric SVD</strong>: Instead of users having their own notion of factor vectors, we can represent users as a bag of items they have rated (or provided implicit feedback for). So Alice is now represented as a (possibly weighted) sum of the factor vectors of the items she has rated, and to get her predicted rating of Titanic, we can dot product this  representation with the factor vector of Titanic. From a practical perspective, this model has an added benefit in that no user parameterizations are needed, so we can use this approach to generate recommendations as soon as a user provides some feedback (which could just be views or clicks on an item, and not necessarily ratings), without needing to retrain the model to factorize the user.</li>
  <li><strong>SVD++</strong>: Incorporate both the standard SVD and the asymmetric SVD model by representing users both by their own factor representation and as a bag of item vectors.</li>
  </ul>


  <h1>Regression</h1>

  <p>Some regression models were also used in the predictions. The models are fairly standard, I think, so I won&#8217;t spend too long here. Basically, just as with the neighborhood models, we can take a user-centric approach and a movie-centric approach to regression:</p>

  <ul>
  <li><strong>User-centric approach</strong>: We learn a regression model for each user, using all the movies that the user rated as the dataset. The response is the movie&#8217;s rating, and the predictor variables are attributes associated to that movie (which can be derived from, say, PCA, MDS, or an SVD).</li>
  <li><strong>Movie-centric approach</strong>: Similarly, we can learn a regression model for each movie, using all the users that rated the movie as the dataset.</li>
  </ul>


  <h1>Restricted Boltzmann Machines</h1>

  <p>Restricted Boltzmann Machines provide another kind of <strong>latent factor approach</strong> that can be used. See <a href="http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf">this paper</a> for a description of how to apply them to the Netflix Prize. (In case the paper&#8217;s a little difficult to read, I wrote an <a href="http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines">introduction to RBMs</a> a little while ago.)</p>

  <h1>Temporal Effects</h1>

  <p>Many of the models incorporate temporal effects. For example, when describing the baseline predictors above, we used a few temporal predictors that allowed a user&#8217;s rating to (linearly) depend on the time since the first rating he ever made and on the time since a movie&#8217;s first rating. We can also get more fine-grained temporal effects by, say, binning items into a couple months&#8217; worth of ratings at a time, and allowing movie biases to change within each bin. (For example, maybe in May 2006, Time Magazine nominated Titanic as the best movie ever made, which caused a spurt in glowing ratings around that time.)</p>

  <p>In the matrix factorization approach, user factors were also allowed to be time-dependent (e.g., maybe Bob comes to like comedy movies more and more over time). We can also give more weight to recent user actions.</p>

  <h1>Regularization</h1>

  <p>Regularization was also applied throughout pretty much all the models learned, to <strong>prevent overfitting</strong> on the dataset. Ridge regression was heavily used in the factorization models to penalize large weights, and lasso regression (though less effective) was useful as well. Many other parameters (e.g., the baseline predictors, similarity weights and interpolation weights in the neighborhood models) were also estimated using fairly standard shrinkage techniques.</p>

  <h1>Ensemble Methods</h1>

  <p>Finally, let&#8217;s talk about how all of these different algorithms were combined to provide a single rating that <strong>exploits the strengths of each model</strong>. (Note that, as mentioned above, many of these models were not trained on the raw ratings data directly, but rather on the residuals of other models.)</p>

  <p>In the paper detailing their final solution, the winners describe using <strong>gradient boosted decision trees to combine over 500 models</strong>; previous solutions used instead a <strong>linear regression</strong> to combine the predictors.</p>

  <p>Briefly, gradient boosted decision trees work by sequentially fitting a series of decision trees to the data; each tree is asked to predict the error made by the previous trees, and is often trained on slightly perturbed versions of the data. (For a longer description of a similar technique, see <a href="http://blog.echen.me/2011/03/14/laymans-introduction-to-random-forests/">my introduction to random forests</a>.)</p>

  <p>Since GBDTs have a built-in ability to apply different methods to different slices of the data, we can add in some predictors that help the trees make useful clusterings:</p>

  <ul>
  <li>Number of movies each user rated</li>
  <li>Number of users that rated each movie</li>
  <li>Factor vectors of users and movies</li>
  <li>Hidden units of a restricted Boltzmann Machine</li>
  </ul>


  <p>(For example, one thing that Bell and Koren found (when using an earlier ensemble method) was that RBMs are more useful when the movie or the user has a low number of ratings, and that matrix factorization methods are more useful when the movie or user has a high number of ratings.)</p>

  <p>Here&#8217;s a graph of the effect of ensemble size from early on in the competition (in 2007), and the authors&#8217; take on it:</p>

  <p><a href="http://www2.research.att.com/~volinsky/netflix/newensemble.gif"><img src="http://www2.research.att.com/~volinsky/netflix/newensemble.gif" alt="Ensemble Size vs. RMSE" /></a></p>

  <blockquote><p>However, we would like to stress that it is not necessary to have such a large number of models to do well. The plot below shows RMSE as a function of the number of methods used. One can achieve our winning score (RMSE=0.8712) with less than 50 methods, using the best 3 methods can yield RMSE < 0.8800, which would land in the top 10. Even just using our single best method puts us on the leaderboard with an RMSE of 0.8890. The lesson here is that having lots of models is useful for the incremental results needed to win competitions, but practically, excellent systems can be built with just a few well-selected models.</p></blockquote>

  </div>  

                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/2011/09/29/stuff-harvard-people-like/">Stuff Harvard People Like</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/edwin-chen.html">Edwin Chen</a>
        </li>
        <li class="published" title="2011-09-29T04:15:00">
          on&nbsp;Thu 29 September 2011
        </li>

	</ul>

</div><!-- /.post-info -->
  <div class="entry-content"><p>What types of students go to which schools? There are, of course, the classic stereotypes:</p>

  <ul>
  <li><strong>MIT</strong> has the hacker engineers.</li>
  <li><strong>Stanford</strong> has the laid-back, social folks.</li>
  <li><strong>Harvard</strong> has the prestigious leaders of the world.</li>
  <li><strong>Berkeley</strong> has the activist hippies.</li>
  <li><strong>Caltech</strong> has the hardcore science nerds.</li>
  </ul>


  <p>But how well do these perceptions match reality? What are students at Stanford, Harvard, MIT, Caltech, and Berkeley <em>really</em> interested in? Following the path of my previous data-driven post on <a href="http://blog.echen.me/2011/04/18/twifferences-between-californians-and-new-yorkers/">differences between Silicon Valley and NYC</a>, I scraped the Quora profiles of a couple hundred followers of each school to find out.</p>

  <h1>Topics</h1>

  <p>So let&#8217;s look at what kinds of topics followers of each school are interested in*. (Skip past the lists for a discussion.)</p>

  <h2>MIT</h2>

  <p>Topics are followed by p(school = MIT|topic).</p>

  <ul>
  <li><strong>MIT Media Lab</strong>                             0.893</li>
  <li><strong>Ksplice</strong>                                   0.69</li>
  <li><strong>Lisp (programming language)</strong>               0.677</li>
  <li><strong>Nokia</strong>                                     0.659</li>
  <li><strong>Public Speaking</strong>                           0.65</li>
  <li><strong>Data Storage</strong>                              0.65</li>
  <li><strong>Google Voice</strong>                              0.609</li>
  <li><strong>Hacking</strong>                                   0.602</li>
  <li><strong>Startups in Europe</strong>                        0.597</li>
  <li><strong>Startup Names</strong>                             0.572</li>
  <li><strong>Mechanical Engineering</strong>                    0.563</li>
  <li><strong>Engineering</strong>                               0.563</li>
  <li><strong>Distributed Databases</strong>                     0.544</li>
  <li><strong>StackOverflow</strong>                             0.536</li>
  <li><strong>Boston</strong>                                    0.513</li>
  <li><strong>Learning</strong>                                  0.507</li>
  <li><strong>Open Source</strong>                               0.498</li>
  <li><strong>Cambridge</strong>                                 0.496</li>
  <li><strong>Public Relations</strong>                          0.493</li>
  <li><strong>Visualization</strong>                             0.492</li>
  <li><strong>Semantic Web</strong>                              0.486</li>
  <li><strong>Andreessen-Horowitz</strong>                       0.483</li>
  <li><strong>Nature</strong>                                    0.475</li>
  <li><strong>Cryptography</strong>                              0.474</li>
  <li><strong>Startups in Boston</strong>                        0.452</li>
  <li><strong>Adobe Photoshop</strong>                           0.451</li>
  <li><strong>Computer Security</strong>                         0.447</li>
  <li><strong>Sachin Tendulkar</strong>                          0.443</li>
  <li><strong>Hacker News</strong>                               0.442</li>
  <li><strong>Games</strong>                                     0.429</li>
  <li><strong>Android Applications</strong>                      0.428</li>
  <li><strong>Best Engineers and Programmers</strong>            0.427</li>
  <li><strong>College Admissions &amp; Getting Into College</strong> 0.422</li>
  <li><strong>Co-Founders</strong>                               0.419</li>
  <li><strong>Big Data</strong>                                  0.41</li>
  <li><strong>System Administration</strong>                     0.4</li>
  <li><strong>Biotechnology</strong>                             0.398</li>
  <li><strong>Higher Education</strong>                          0.394</li>
  <li><strong>NoSQL</strong>                                     0.387</li>
  <li><strong>User Experience</strong>                           0.386</li>
  <li><strong>Career Advice</strong>                             0.377</li>
  <li><strong>Artificial Intelligence</strong>                   0.375</li>
  <li><strong>Scalability</strong>                               0.37</li>
  <li><strong>Taylor Swift</strong>                              0.368</li>
  <li><strong>Google Search</strong>                             0.368</li>
  <li><strong>Functional Programming</strong>                    0.365</li>
  <li><strong>Bing</strong>                                      0.363</li>
  <li><strong>Bioinformatics</strong>                            0.361</li>
  <li><strong>How I Met Your Mother (TV series)</strong>         0.361</li>
  <li><strong>Operating Systems</strong>                         0.356</li>
  <li><strong>Compilers</strong>                                 0.355</li>
  <li><strong>Google Chrome</strong>                             0.354</li>
  <li><strong>Management &amp; Organizational Leadership</strong>    0.35</li>
  <li><strong>Literary Fiction</strong>                          0.35</li>
  <li><strong>Intelligence</strong>                              0.348</li>
  <li><strong>Fight Club (1999 movie)</strong>                   0.344</li>
  <li><strong>Hip Hop Music</strong>                             0.34</li>
  <li><strong>UX Design</strong>                                 0.337</li>
  <li><strong>Web Application Frameworks</strong>                0.336</li>
  <li><strong>Startups in New York City</strong>                 0.333</li>
  <li><strong>Book Recommendations</strong>                      0.33</li>
  <li><strong>Engineering Recruiting</strong>                    0.33</li>
  <li><strong>Search Engines</strong>                            0.329</li>
  <li><strong>Social Search</strong>                             0.329</li>
  <li><strong>Data Science</strong>                              0.328</li>
  <li><strong>History</strong>                                   0.328</li>
  <li><strong>Interaction Design</strong>                        0.326</li>
  <li><strong>Classification (machine learning)</strong>         0.322</li>
  <li><strong>Startup Incubators and Seed Programs</strong>      0.321</li>
  <li><strong>Graphic Design</strong>                            0.321</li>
  <li><strong>Product Design (software)</strong>                 0.319</li>
  <li><strong>The College Experience</strong>                    0.319</li>
  <li><strong>Writing</strong>                                   0.319</li>
  <li><strong>MapReduce</strong>                                 0.318</li>
  <li><strong>Database Systems</strong>                          0.315</li>
  <li><strong>User Interfaces</strong>                           0.314</li>
  <li><strong>Literature</strong>                                0.314</li>
  <li><strong>C (programming language)</strong>                  0.314</li>
  <li><strong>Television</strong>                                0.314</li>
  <li><strong>Reading</strong>                                   0.313</li>
  <li><strong>Usability</strong>                                 0.312</li>
  <li><strong>Books</strong>                                     0.312</li>
  <li><strong>Computers</strong>                                 0.311</li>
  <li><strong>Stealth Startups</strong>                          0.311</li>
  <li><strong>Daft Punk</strong>                                 0.31</li>
  <li><strong>Healthy Eating</strong>                            0.309</li>
  <li><strong>Innovation</strong>                                0.309</li>
  <li><strong>Skiing</strong>                                    0.305</li>
  <li><strong>JavaScript</strong>                                0.304</li>
  <li><strong>Rock Music</strong>                                0.304</li>
  <li><strong>Mozilla Firefox</strong>                           0.304</li>
  <li><strong>Self-Improvement</strong>                          0.303</li>
  <li><strong>McKinsey &amp; Company</strong>                        0.302</li>
  <li><strong>AngelList</strong>                                 0.301</li>
  <li><strong>Data Visualization</strong>                        0.301</li>
  <li><strong>Cassandra (database)</strong>                      0.301</li>
  </ul>


  <h2>Stanford</h2>

  <p>Topics are followed by p(school = Stanford|topic).</p>

  <ul>
  <li><strong>Stanford Computer Science</strong>                 0.951</li>
  <li><strong>Stanford Graduate School of Business</strong>      0.939</li>
  <li><strong>Stanford</strong>                                  0.896</li>
  <li><strong>Stanford Football</strong>                         0.896</li>
  <li><strong>Stanford Cardinal</strong>                         0.896</li>
  <li><strong>Social Dance</strong>                              0.847</li>
  <li><strong>Stanford University Courses</strong>               0.847</li>
  <li><strong>Romance</strong>                                   0.769</li>
  <li><strong>Instagram</strong>                                 0.745</li>
  <li><strong>College Football</strong>                          0.665</li>
  <li><strong>Mobile Location Applications</strong>              0.634</li>
  <li><strong>Online Communities</strong>                        0.621</li>
  <li><strong>Interpersonal Relationships</strong>               0.585</li>
  <li><strong>Food &amp; Restaurants in Palo Alto</strong>           0.572</li>
  <li><strong>Your 20s</strong>                                  0.566</li>
  <li><strong>Men&#8217;s Fashion</strong>                             0.548</li>
  <li><strong>Flipboard</strong>                                 0.537</li>
  <li><strong>Inception (2010 movie)</strong>                    0.535</li>
  <li><strong>Tumblr</strong>                                    0.531</li>
  <li><strong>People Skills</strong>                             0.522</li>
  <li><strong>Exercise</strong>                                  0.52</li>
  <li><strong>Joel Spolsky</strong>                              0.516</li>
  <li><strong>Valuations</strong>                                0.515</li>
  <li><strong>The Social Network (2010 movie)</strong>           0.513</li>
  <li><strong>LeBron James</strong>                              0.506</li>
  <li><strong>Northern California</strong>                       0.506</li>
  <li><strong>Evernote</strong>                                  0.5</li>
  <li><strong>Quora Community</strong>                           0.5</li>
  <li><strong>Blogging</strong>                                  0.49</li>
  <li><strong>Downtown Palo Alto</strong>                        0.487</li>
  <li><strong>The College Experience</strong>                    0.485</li>
  <li><strong>Consumer Internet</strong>                         0.477</li>
  <li><strong>Restaurants in San Francisco</strong>              0.477</li>
  <li><strong>Chad Hurley</strong>                               0.47</li>
  <li><strong>Meditation</strong>                                0.468</li>
  <li><strong>Yishan Wong</strong>                               0.466</li>
  <li><strong>Arrested Development (TV series)</strong>          0.463</li>
  <li><strong>fbFund</strong>                                    0.457</li>
  <li><strong>Best Engineers at X Company</strong>               0.451</li>
  <li><strong>Language</strong>                                  0.45</li>
  <li><strong>Words</strong>                                     0.448</li>
  <li><strong>Happiness</strong>                                 0.447</li>
  <li><strong>Path (company)</strong>                            0.446</li>
  <li><strong>Color Labs (startup)</strong>                      0.446</li>
  <li><strong>Palo Alto</strong>                                 0.445</li>
  <li><strong>Woot.com</strong>                                  0.442</li>
  <li><strong>Beer</strong>                                      0.442</li>
  <li><strong>PayPal</strong>                                    0.441</li>
  <li><strong>Women in Startups</strong>                         0.438</li>
  <li><strong>Techmeme</strong>                                  0.433</li>
  <li><strong>Women in Engineering</strong>                      0.428</li>
  <li><strong>The Mission (San Francisco neighborhood)</strong>  0.427</li>
  <li><strong>iPhone Applications</strong>                       0.416</li>
  <li><strong>Asana</strong>                                     0.413</li>
  <li><strong>Monetization</strong>                              0.412</li>
  <li><strong>Repetitive Strain Injury (RSI)</strong>            0.4</li>
  <li><strong>IDEO</strong>                                      0.398</li>
  <li><strong>Spotify</strong>                                   0.397</li>
  <li><strong>San Francisco Giants</strong>                      0.396</li>
  <li><strong>Fortune Magazine</strong>                          0.389</li>
  <li><strong>Love</strong>                                      0.387</li>
  <li><strong>Human-Computer Interaction</strong>                0.382</li>
  <li><strong>Hip Hop Music</strong>                             0.378</li>
  <li><strong>Self-Improvement</strong>                          0.378</li>
  <li><strong>Food in San Francisco</strong>                     0.375</li>
  <li><strong>Quora (company)</strong>                           0.374</li>
  <li><strong>Quora Infrastructure</strong>                      0.373</li>
  <li><strong>iPhone</strong>                                    0.371</li>
  <li><strong>Square (company)</strong>                          0.369</li>
  <li><strong>Social Psychology</strong>                         0.369</li>
  <li><strong>Network Effects</strong>                           0.366</li>
  <li><strong>Chris Sacca</strong>                               0.365</li>
  <li><strong>Walt Mossberg</strong>                             0.364</li>
  <li><strong>Salesforce.com</strong>                            0.362</li>
  <li><strong>Sex</strong>                                       0.361</li>
  <li><strong>Etiquette</strong>                                 0.361</li>
  <li><strong>David Pogue</strong>                               0.361</li>
  <li><strong>Gowalla</strong>                                   0.36</li>
  <li><strong>iOS Development</strong>                           0.354</li>
  <li><strong>Palantir Technologies</strong>                     0.353</li>
  <li><strong>Mobile Computing</strong>                          0.347</li>
  <li><strong>Sports</strong>                                    0.346</li>
  <li><strong>Video Games</strong>                               0.345</li>
  <li><strong>Burning Man</strong>                               0.345</li>
  <li><strong>Engineering Management</strong>                    0.343</li>
  <li><strong>Cognitive Science</strong>                         0.342</li>
  <li><strong>Dating &amp; Relationships</strong>                    0.341</li>
  <li><strong>Fred Wilson (venture investor)</strong>            0.337</li>
  <li><strong>Taiwan</strong>                                    0.333</li>
  <li><strong>Natural Language Processing</strong>               0.33</li>
  <li><strong>Eric Schmidt</strong>                              0.329</li>
  <li><strong>Social Advice</strong>                             0.329</li>
  <li><strong>Engineering Recruiting</strong>                    0.328</li>
  <li><strong>Job Interviews</strong>                            0.325</li>
  <li><strong>Mobile Phones</strong>                             0.324</li>
  <li><strong>Twitter Inc. (company)</strong>                    0.321</li>
  <li><strong>Engineering in Silicon Valley</strong>             0.321</li>
  <li><strong>San Francisco Bay Area</strong>                    0.321</li>
  <li><strong>Google Analytics</strong>                          0.32</li>
  <li><strong>Fashion</strong>                                   0.315</li>
  <li><strong>Interaction Design</strong>                        0.314</li>
  <li><strong>Open Graph</strong>                                0.313</li>
  <li><strong>Drugs &amp; Pharmaceuticals</strong>                   0.312</li>
  <li><strong>Electronic Music</strong>                          0.312</li>
  <li><strong>Facebook Inc. (company)</strong>                   0.309</li>
  <li><strong>Fitness</strong>                                   0.309</li>
  <li><strong>YouTube</strong>                                   0.308</li>
  <li><strong>TED Talks</strong>                                 0.308</li>
  <li><strong>Freakonomics (2005 Book)</strong>                  0.307</li>
  <li><strong>Jack Dorsey</strong>                               0.306</li>
  <li><strong>Nutrition</strong>                                 0.305</li>
  <li><strong>Puzzles</strong>                                   0.305</li>
  <li><strong>Silicon Valley Mergers &amp; Acquisitions</strong>     0.304</li>
  <li><strong>Viral Growth &amp; Analytics</strong>                  0.304</li>
  <li><strong>Amazon Web Services</strong>                       0.304</li>
  <li><strong>StumbleUpon</strong>                               0.303</li>
  <li><strong>Exceptional Comment Threads</strong>               0.303</li>
  </ul>


  <h2>Harvard</h2>

  <ul>
  <li><strong>Harvard Business School</strong>                   0.968</li>
  <li><strong>Harvard Business Review</strong>                   0.922</li>
  <li><strong>Harvard Square</strong>                            0.912</li>
  <li><strong>Harvard Law School</strong>                        0.912</li>
  <li><strong>Jimmy Fallon</strong>                              0.899</li>
  <li><strong>Boston Red Sox</strong>                            0.658</li>
  <li><strong>Klout</strong>                                     0.644</li>
  <li><strong>Oprah Winfrey</strong>                             0.596</li>
  <li><strong>Ivanka Trump</strong>                              0.587</li>
  <li><strong>Dalai Lama</strong>                                0.569</li>
  <li><strong>Food in New York City</strong>                     0.565</li>
  <li><strong>U2</strong>                                        0.562</li>
  <li><strong>TwitPic</strong>                                   0.534</li>
  <li><strong>37signals</strong>                                 0.522</li>
  <li><strong>David Lynch (director)</strong>                    0.512</li>
  <li><strong>Al Gore</strong>                                   0.508</li>
  <li><strong>TechStars</strong>                                 0.49</li>
  <li><strong>Baseball</strong>                                  0.487</li>
  <li><strong>Private Equity</strong>                            0.471</li>
  <li><strong>Classical Music</strong>                           0.46</li>
  <li><strong>Startups in New York City</strong>                 0.458</li>
  <li><strong>HootSuite</strong>                                 0.449</li>
  <li><strong>Kiva</strong>                                      0.442</li>
  <li><strong>Ultimate Frisbee</strong>                          0.441</li>
  <li><strong>Huffington Post</strong>                           0.436</li>
  <li><strong>New York City</strong>                             0.433</li>
  <li><strong>Charlie Cheever</strong>                           0.433</li>
  <li><strong>The New York Times</strong>                        0.431</li>
  <li><strong>Technology Journalism</strong>                     0.431</li>
  <li><strong>McKinsey &amp; Company</strong>                        0.427</li>
  <li><strong>TweetDeck</strong>                                 0.422</li>
  <li><strong>How Does X Work?</strong>                          0.417</li>
  <li><strong>Ashton Kutcher</strong>                            0.414</li>
  <li><strong>Coldplay</strong>                                  0.402</li>
  <li><strong>Conan O&#8217;Brien</strong>                             0.397</li>
  <li><strong>Fast Company</strong>                              0.397</li>
  <li><strong>WikiLeaks</strong>                                 0.394</li>
  <li><strong>Michael Jackson</strong>                           0.389</li>
  <li><strong>Guy Kawasaki</strong>                              0.389</li>
  <li><strong>Journalism</strong>                                0.384</li>
  <li><strong>Wall Street Journal</strong>                       0.384</li>
  <li><strong>Cambridge</strong>                                 0.371</li>
  <li><strong>Seattle</strong>                                   0.37</li>
  <li><strong>Cities &amp; Metro Areas</strong>                      0.357</li>
  <li><strong>Boston</strong>                                    0.353</li>
  <li><strong>Tim Ferriss (author)</strong>                      0.35</li>
  <li><strong>The New Yorker</strong>                            0.343</li>
  <li><strong>Law</strong>                                       0.34</li>
  <li><strong>Mashable</strong>                                  0.338</li>
  <li><strong>Politics</strong>                                  0.335</li>
  <li><strong>The Economist</strong>                             0.334</li>
  <li><strong>Barack Obama</strong>                              0.333</li>
  <li><strong>Skiing</strong>                                    0.329</li>
  <li><strong>McKinsey Quarterly</strong>                        0.325</li>
  <li><strong>Wired (magazine)</strong>                          0.316</li>
  <li><strong>Bill Gates</strong>                                0.31</li>
  <li><strong>Mad Men (TV series)</strong>                       0.308</li>
  <li><strong>India</strong>                                     0.306</li>
  <li><strong>TED Talks</strong>                                 0.306</li>
  <li><strong>Netflix</strong>                                   0.304</li>
  <li><strong>Wine</strong>                                      0.303</li>
  <li><strong>Angel Investors</strong>                           0.302</li>
  <li><strong>Facebook Ads</strong>                              0.301</li>
  </ul>


  <h2>UC Berkeley</h2>

  <ul>
  <li><strong>Berkeley</strong>                                  0.978</li>
  <li><strong>California Golden Bears</strong>                   0.91</li>
  <li><strong>Internships</strong>                               0.717</li>
  <li><strong>Web Marketing</strong>                             0.484</li>
  <li><strong>Google Social Strategy</strong>                    0.453</li>
  <li><strong>Southwest Airlines</strong>                        0.451</li>
  <li><strong>WordPress</strong>                                 0.429</li>
  <li><strong>Stock Market</strong>                              0.429</li>
  <li><strong>BMW (automobile)</strong>                          0.428</li>
  <li><strong>Web Applications</strong>                          0.423</li>
  <li><strong>Flickr</strong>                                    0.422</li>
  <li><strong>Snowboarding</strong>                              0.42</li>
  <li><strong>Electronic Music</strong>                          0.404</li>
  <li><strong>MySQL</strong>                                     0.401</li>
  <li><strong>Internet Advertising</strong>                      0.399</li>
  <li><strong>Search Engine Optimization (SEO)</strong>          0.398</li>
  <li><strong>Yelp</strong>                                      0.396</li>
  <li><strong>Groupon</strong>                                   0.393</li>
  <li><strong>In-N-Out Burger</strong>                           0.391</li>
  <li><strong>The Matrix (1999 movie)</strong>                   0.389</li>
  <li><strong>Trading (finance)</strong>                         0.385</li>
  <li><strong>jQuery</strong>                                    0.381</li>
  <li><strong>Hedge Funds</strong>                               0.378</li>
  <li><strong>Social Media Marketing</strong>                    0.377</li>
  <li><strong>San Francisco</strong>                             0.376</li>
  <li><strong>Stealth Startups</strong>                          0.362</li>
  <li><strong>Yahoo!</strong>                                    0.36</li>
  <li><strong>Cascading Style Sheets</strong>                    0.359</li>
  <li><strong>Angel Investors</strong>                           0.355</li>
  <li><strong>UX Design</strong>                                 0.35</li>
  <li><strong>StarCraft</strong>                                 0.348</li>
  <li><strong>Los Angeles Lakers</strong>                        0.347</li>
  <li><strong>Mountain View</strong>                             0.345</li>
  <li><strong>How I Met Your Mother (TV series)</strong>         0.338</li>
  <li><strong>Google+</strong>                                   0.337</li>
  <li><strong>Ruby on Rails</strong>                             0.333</li>
  <li><strong>Reading</strong>                                   0.333</li>
  <li><strong>Social Media</strong>                              0.326</li>
  <li><strong>China</strong>                                     0.322</li>
  <li><strong>Palantir Technologies</strong>                     0.319</li>
  <li><strong>Facebook Platform</strong>                         0.315</li>
  <li><strong>Basketball</strong>                                0.315</li>
  <li><strong>Education</strong>                                 0.314</li>
  <li><strong>Business Development</strong>                      0.312</li>
  <li><strong>Online &amp; Mobile Payments</strong>                  0.305</li>
  <li><strong>Restaurants in San Francisco</strong>              0.302</li>
  <li><strong>Technology Companies</strong>                      0.302</li>
  <li><strong>Seth Godin</strong>                                0.3</li>
  </ul>


  <h2>Caltech</h2>

  <ul>
  <li><strong>Pasadena</strong>                                  0.969</li>
  <li><strong>Chess</strong>                                     0.748</li>
  <li><strong>Table Tennis</strong>                              0.671</li>
  <li><strong>UCLA</strong>                                      0.67</li>
  <li><strong>MacBook Pro</strong>                               0.618</li>
  <li><strong>Physics</strong>                                   0.618</li>
  <li><strong>Haskell</strong>                                   0.582</li>
  <li><strong>Los Angeles</strong>                               0.58</li>
  <li><strong>Electrical Engineering</strong>                    0.567</li>
  <li><strong>Star Trek (movie</strong>                          0.561</li>
  <li><strong>Disruptive Technology</strong>                     0.545</li>
  <li><strong>Science</strong>                                   0.53</li>
  <li><strong>Biology</strong>                                   0.526</li>
  <li><strong>Quantum Mechanics</strong>                         0.521</li>
  <li><strong>LaTeX</strong>                                     0.514</li>
  <li><strong>Mathematics</strong>                               0.488</li>
  <li><strong>xkcd</strong>                                      0.488</li>
  <li><strong>Genetics &amp; Heredity</strong>                       0.487</li>
  <li><strong>Chemistry</strong>                                 0.47</li>
  <li><strong>Medicine &amp; Healthcare</strong>                     0.448</li>
  <li><strong>Poker</strong>                                     0.445</li>
  <li><strong>C++ (programming language)</strong>                0.442</li>
  <li><strong>Data Structures</strong>                           0.434</li>
  <li><strong>Emacs</strong>                                     0.428</li>
  <li><strong>MongoDB</strong>                                   0.423</li>
  <li><strong>Neuroscience</strong>                              0.404</li>
  <li><strong>Science Fiction</strong>                           0.4</li>
  <li><strong>Mac OS X</strong>                                  0.394</li>
  <li><strong>Board Games</strong>                               0.387</li>
  <li><strong>Computers</strong>                                 0.386</li>
  <li><strong>Research</strong>                                  0.385</li>
  <li><strong>Finance</strong>                                   0.385</li>
  <li><strong>The Future</strong>                                0.379</li>
  <li><strong>Linux</strong>                                     0.378</li>
  <li><strong>The Colbert Report</strong>                        0.376</li>
  <li><strong>The Beatles</strong>                               0.374</li>
  <li><strong>The Onion</strong>                                 0.365</li>
  <li><strong>Ruby</strong>                                      0.363</li>
  <li><strong>Cars &amp; Automobiles</strong>                        0.361</li>
  <li><strong>Quantitative Finance</strong>                      0.359</li>
  <li><strong>Academia</strong>                                  0.359</li>
  <li><strong>Law</strong>                                       0.355</li>
  <li><strong>Cooking</strong>                                   0.354</li>
  <li><strong>Psychology</strong>                                0.349</li>
  <li><strong>Eminem</strong>                                    0.347</li>
  <li><strong>Football (Soccer)</strong>                         0.346</li>
  <li><strong>Computer Programming</strong>                      0.343</li>
  <li><strong>Algorithms</strong>                                0.343</li>
  <li><strong>Evolutionary Biology</strong>                      0.337</li>
  <li><strong>Behavioral Economics</strong>                      0.335</li>
  <li><strong>California</strong>                                0.329</li>
  <li><strong>Machine Learning</strong>                          0.326</li>
  <li><strong>Futurama</strong>                                  0.324</li>
  <li><strong>Social Advice</strong>                             0.324</li>
  <li><strong>StarCraft II</strong>                              0.319</li>
  <li><strong>Job Interview Questions</strong>                   0.318</li>
  <li><strong>Game Theory</strong>                               0.316</li>
  <li><strong>This American Life</strong>                        0.315</li>
  <li><strong>Economics</strong>                                 0.314</li>
  <li><strong>Vim</strong>                                       0.31</li>
  <li><strong>Graduate School</strong>                           0.309</li>
  <li><strong>Git (revision control)</strong>                    0.306</li>
  <li><strong>Computer Science</strong>                          0.303</li>
  </ul>


  <p>What do we see?</p>

  <ul>
  <li>First, in a nice validation of this approach, we find that each school is interested in exactly the <strong>locations</strong> we&#8217;d expect: Caltech is interested in <em>Pasadena</em> and <em>Los Angeles</em>; MIT and Harvard are both interested in <em>Boston</em> and <em>Cambridge</em> (Harvard is interested in <em>New York City</em> as well); Stanford is interested in <em>Palo Alto</em>, <em>Northern California</em>, and <em>San Francisco Bay Area</em>; and Berkeley is interested in <em>Berkeley</em>, <em>San Francisco</em>, and <em>Mountain View</em>.</li>
  <li>More interestingly, let&#8217;s look at where each school likes to <strong>eat</strong>. Stereotypically, we expect Harvard, Stanford, and Berkeley students to be more outgoing and social, and MIT and Caltech students to be more introverted. This is indeed what we find:

  <ul>
  <li>Harvard follows <em>Food in New York City</em>; Stanford follows <em>Food &amp; Restaurants in Palo Alto</em>, <em>Restaurants in San Francisco</em>, and <em>Food in San Francisco</em>; and Berkeley follows <em>Restaurants in San Francisco</em> and <em>In-N-Out Burger</em>. In other words, Harvard, Stanford, and Berkeley love eating out.</li>
  <li>Caltech, on the other hand, loves <em>Cooking</em>, and MIT loves <em>Healthy Eating</em> &#8211; both signs, perhaps, of a preference for eating in.</li>
  </ul>
  </li>
  <li>And what does each university use to quench their <strong>thirst</strong>? Harvard students like to drink <em>wine</em> (classy!), while Stanford students prefer <em>beer</em> (the social drink of choice).</li>
  <li>What about <strong>sports teams</strong>? MIT and Caltech couldn&#8217;t care less, though Harvard follows the <em>Boston Red Sox</em>, Stanford follows the <em>San Francisco Giants</em> (as well as their own <em>Stanford Football</em> and <em>Stanford Cardinal</em>), and Berkeley follows the <em>Los Angeles Lakers</em> (and the <em>California Golden Bears</em>).</li>
  <li>For <strong>sports</strong> themselves, MIT students like <em>skiing</em>; Stanford students like <em>general exercise</em>, <em>fitness</em>, and <em>sports</em>; Harvard students like <em>baseball</em>, <em>ultimate frisbee</em>, and <em>skiing</em>; and Berkeley students like <em>snowboarding</em>. Caltech, in a league of its own, enjoys <em>table tennis</em> and <em>chess</em>.</li>
  <li>What does each school think of <strong>social</strong>? Caltech students look for Social <em>Advice</em>. Berkeley students are interested in Social <em>Media</em> and Social Media Marketing. MIT, on the more technical side, wants Social <em>Search</em>. Stanford students, predictably, love the whole spectrum of social offerings, from Social <em>Dance</em> and <em>The Social Network</em>, to Social <em>Psychology</em> and Social <em>Advice</em>. (Interestingly, Caltech and Stanford are both interested in Social Advice, though I wonder if it&#8217;s for slightly different reasons.)</li>
  <li>What&#8217;s each school&#8217;s relationship with <strong>computers</strong>? Caltech students are interested in Computer <em>Science</em>, MIT hackers are interested in Computer <em>Security</em>, and Stanford students are interested in Human-Computer <em>Interaction</em>.</li>
  <li>Digging into the <strong>MIT vs. Caltech</strong> divide a little, we see that Caltech students really are more interested in the pure sciences (<em>Physics, Science, Biology, Quantum Mechanics, Mathematics, Chemistry</em>, etc.), while MIT students are more on the applied and engineering sides (<em>Mechanical Engineering, Engineering, Distributed Databases, Cryptography, Computer Security, Biotechnology, Operating Systems, Compilers</em>, etc.).</li>
  <li>Regarding <strong>programming languages</strong>, Caltech students love <em>Haskell</em> (hardcore purity!), while MIT students love <em>Lisp</em>.</li>
  <li>What does each school like to <strong>read</strong>, both offline and online? Caltech loves <em>science fiction</em>, <em>xkcd</em>, and <em>The Onion</em>; MIT likes <em>Hacker News</em>; Harvard loves journals, newspapers, and magazines (<em>Huffington Post</em>, the <em><a href="http://stuffwhitepeoplelike.com/2008/01/31/45-the-sunday-new-york-times/">New York Times</a></em>, <em>Fortune, Wall Street Journal, the New Yorker, the Economist</em>, and so on); and Stanford likes <em>TechMeme</em>.</li>
  <li>What <strong>movies and television shows</strong> does each school like to watch? Caltech likes <em>Star Trek</em>, the <em>Colbert Report</em>, and <em>Futurama</em>. MIT likes <em>Fight Club</em> (I don&#8217;t know what this has to do with MIT, though I will note that on my first day as a freshman in a new dorm, Fight Club was precisely the movie we all went to a lecture hall to see). Stanford likes <em>The Social Network</em> and <em>Inception</em>. Harvard, rather fittingly, likes <em><a href="http://stuffwhitepeoplelike.com/2009/03/11/123-mad-men/">Mad Men</a></em> and <em><a href="http://stuffwhitepeoplelike.com/2010/09/08/134-the-ted-conference/">Ted Talks</a></em>.</li>
  <li>Let&#8217;s look at the <strong>startups</strong> each school follows. MIT, of course, likes <em>Ksplice</em>. Berkeley likes <em>Yelp</em> and <em>Groupon</em>. Stanford likes just about every startup under the sun (<em>Instagram, Flipboard, Tumblr, Path, Color Labs</em>, etc.). And Harvard, that bastion of hard-won influence and prestige? To the surprise of precisely no one, Harvard enjoys <em>Klout</em>.</li>
  </ul>


  <p>Let&#8217;s end with a summarized view of each school:</p>

  <ul>
  <li><strong>Caltech</strong> is very much into the sciences (<em>Physics, Biology, Quantum Mechanics, Mathematics</em>, etc.), as well as many pretty nerdy topics (<em>Star Trek, Science Fiction, xkcd, Futurama, Starcraft II</em>, etc.).</li>
  <li><strong>MIT</strong> is dominated by everything engineering and tech.</li>
  <li><strong>Stanford</strong> loves relationships (<em>interpersonal relationships, people skills, love, network effects, sex, etiquette, dating and relationships, romance</em>), health and appearance (<em>fashion, fitness, nutrition, happiness</em>), and startups (<em>Instagram, Flipboard, Path, Color Labs</em>, etc.).</li>
  <li><strong>Berkeley</strong>, sadly, is perhaps too large and diverse for an overall characterization.</li>
  <li><strong>Harvard</strong> students are fascinated by famous figures (<em>Jimmy Fallon, Oprah Winfrey, Invaka Trump, Dalai Lama, David Lynch, Al Gore, Bill Gates, Barack Obama</em>), and by prestigious newspapers, journals, and magazines (<em>Fortune, the New York Times, the Wall Street Journal, the Economist</em>, and so on). Other very fitting interests include <em><a href="http://stuffwhitepeoplelike.com/2008/01/21/12-non-profit-organizations/">Kiva</a>, <a href="http://stuffwhitepeoplelike.com/2008/09/01/108-appearing-to-enjoy-classical-music/">classical music</a></em>, and <em><a href="http://www.vanityfair.com/online/daily/2008/06/coldplay">Coldplay</a></em>.</li>
  </ul>


  <p>*I pulled about 400 followers from each school, and added a couple filters, to try to ensure that followers were actual attendees of the schools rather than general people simply interested in them. Topics are sorted using a naive Bayes score and filtered to have at least 5 counts. Also, a word of warning: my dataset was fairly small and users on Quora are almost certainly not representative of their schools as a whole (though I tried to be rigorous with what I had).</p>
  </div>  

                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/2011/09/07/information-transmission-in-a-social-network-dissecting-the-spread-of-a-quora-post/">Information Transmission in a Social Network: Dissecting the Spread of a Quora Post</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/edwin-chen.html">Edwin Chen</a>
        </li>
        <li class="published" title="2011-09-07T04:15:00">
          on&nbsp;Wed 07 September 2011
        </li>

	</ul>

</div><!-- /.post-info -->
  <div class="entry-content"><p><strong>tl;dr</strong> See <a href="http://www.youtube.com/watch?v=cZ4Ntg4jQHw">this movie visualization</a> for a case study on how a post propagates through Quora.</p>

  <p>How does information spread through a network? Much of Quora&#8217;s appeal, after all, lies in its social graph &#8211; and when you&#8217;ve got a network of users, all broadcasting their activities to their neighbors, information can cascade in multiple ways. How do these social designs affect which users see what?</p>

  <p>Think, for example, of what happens when your kid learns a new slang word at school. He doesn&#8217;t confine his use of the word to McKinley Elementary&#8217;s particular boundaries, between the times of 9-3pm &#8211; he introduces it to his friends from other schools at soccer practice as well. A couple months later, he even says it at home for the first time; you like the word so much, you then start using it at work. Eventually, Justin Bieber uses the word in a song, at which point the word&#8217;s popularity really starts to explode.</p>

  <p>So how does information propagate through a social network? What types of people does an answer on Quora reach, and how does it reach them? (Do users discover new answers individually, or are hubs of connectors more key?) How does the activity of a post on Quora rise and fall? (Submissions on other sites have limited lifetimes, fading into obscurity soon after an initial spike; how does that change when users are connected and every upvote can revive a post for someone else&#8217;s eyes?)</p>

  <p>(I looked at Quora since I had some data from there already available, but I hope the lessons should be fairly applicable in general, to other social networks like Facebook, Twitter, and LinkedIn as well.)</p>

  <p>To give an initial answer to some of these questions, I dug into one of my more popular posts, on <a href="http://www.quora.com/Random-Forests/How-do-random-forests-work-in-laymans-terms">a layman&#8217;s introduction to random forests</a>.</p>

  <h1>Users, Topics</h1>

  <p>Before looking deeper into the voting dynamics of the post, let&#8217;s first get some background on what kinds of users the answer reached.</p>

  <p>Here&#8217;s a graph of the topics that question upvoters follow. (Each node is a topic, and every time upvoter X follows both topics A and B, I add an edge between A and B.)</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoter-topics-unlabeled.png"><img src="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoter-topics-unlabeled.png" alt="Upvoters' Topics - Unlabeled" /></a></p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoter-topics-labeled.png"><img src="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoter-topics-labeled.png" alt="Upvoters' Topics - Labeled" /></a></p>

  <p>We can see from the graph that upvoters tend to be interested in three kinds of topics:</p>

  <ul>
  <li><strong>Machine learning and other technical matters</strong> (the green cluster): Classification, Data Mining, Big Data, Information Retrieval, Analytics, Probability, Support Vector Machines, R, Data Science, &#8230;</li>
  <li><strong>Startups/Silicon Valley</strong> (the red cluster): Facebook, Lean Startups, Investing, Seed Funding, Angel Investing, Technology Trends, Product Managment, Silicon Valley Mergers and Acquisitions, Asana, Social Games, Quora, Mark Zuckerberg, User Experience, Founders and Entrepreneurs, &#8230;</li>
  <li><strong>General Intellectual Topics</strong> (the purple cluster): TED, Science, Book Recommendations, Philosophy, Politics, Self-Improvement, Travel, Life Hacks, &#8230;</li>
  </ul>


  <p>Also, here&#8217;s the network of the upvoters themselves (there&#8217;s an edge between users A and B if A follows B):</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoters-unlabeled.png"><img src="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoters-unlabeled.png" alt="Upvote Network - Unlabeled" /></a></p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoters-labeled.png"><img src="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoters-labeled.png" alt="Upvote Network - Labeled" /></a></p>

  <p>We can see three main clusters of users:</p>

  <ul>
  <li>A large group in <strong>green</strong> centered around a lot of power users and Quora employees.</li>
  <li>A machine learning group of folks in <strong>orange</strong> centered around people like Oliver Grisel, Christian Langreiter, and Joseph Turian.</li>
  <li>A group of people following me, in <strong>purple</strong>.</li>
  <li>Plus some smaller clusters in blue and yellow. (There were also a bunch of isolated users, connected to no one, that I filtered out of the picture.)</li>
  </ul>


  <p>Digging into how these topic and user graphs are related:</p>

  <ul>
  <li>The orange cluster of users is more heavily into machine learning: 79% of users in that cluster follow more green topics (machine learning and technical topics) than red and purple topics (startups and general intellectual matters).</li>
  <li>The green cluster of users is reversed: 77% of users follow more of the red and purple clusters of topics (on startups and general intellectual matters) than machine learning and technical topics.</li>
  </ul>


  <p>More interestingly, though, we can ask: how do the connections between upvoters relate to the way the post spread?</p>

  <h1>Social Voting Dynamics</h1>

  <p>So let&#8217;s take a look. Here&#8217;s a visualization I made of upvotes on my answer across time (click <a href="http://www.youtube.com/watch?v=cZ4Ntg4jQHw">here</a> for a larger view).</p>

  <iframe width="640" height="510" src="http://www.youtube.com/embed/cZ4Ntg4jQHw " frameborder="0" allowfullscreen></iframe>


  <p></p>

  <p>To represent the social dynamics of these upvotes, I drew an edge from user A to user B if user A transmitted the post to user B through an upvote. (Specifically, I drew an edge from Alice to Bob if Bob follows Alice and Bob&#8217;s upvote appeared within five days of Alice&#8217;s upvote; this is meant to simulate the idea that Alice was the key intermediary between my post and Bob.)</p>

  <p>Also,</p>

  <ul>
  <li>Green nodes are users with at least one upvote edge.</li>
  <li>Blue nodes are users who follow at least one of the topics the post is categorized under (i.e., users who probably discovered the answer by themselves).</li>
  <li>Red nodes are users with no connections and who do not follow any of the post&#8217;s topics (i.e, users whose path to the post remain mysterious).</li>
  <li>Users increase in size when they produce more connections.</li>
  </ul>


  <p>Here&#8217;s a play-by-play of the video:</p>

  <ul>
  <li>On Feb 14 (the day I wrote the answer), there&#8217;s a flurry of activity.</li>
  <li>A couple of days later, Tracy Chou gives an upvote, leading to another spike in activity.</li>
  <li>Then all&#8217;s quiet until&#8230; bam! Alex Kamil leads to a surge of upvotes, and his upvote finds Ludi Rehak, who starts a small surge of her own. They&#8217;re quickly followed by Christian Langreiter, who starts a small revolution among a bunch of machine learning folks a couple days later.</li>
  <li>Then all is pretty calm again, until a couple months later when&#8230; bam! Aditya Sengupta brings in a smashing of his own followers, and his upvote makes its way to Marc Bodnick, who sets off a veritable storm of activity.</li>
  </ul>


  <p>(Already we can see some relationships between the graph of user connections and the way the post propagated. Many of the users from the orange cluster, for example, come from Alex Kamil and Christian Langreiter&#8217;s upvotes, and many of the users from the green cluster come from Aditya Sengupta and Marc Bodnick&#8217;s upvotes. What&#8217;s interesting, though, is, why didn&#8217;t the cluster of green users appear all at once, like the orange cluster did? People like Kah Seng Tay, Tracy Chou, Venkatesh Rao, and Chad Little upvoted the answer pretty early on, but it wasn&#8217;t until Aditya Sengupta&#8217;s upvote a couple months later that people like Marc Bodnick, Edmond Lau, and many of the other green users (who do indeed follow that first set of folks) discovered the answer. Did the post simply get lost in users&#8217; feeds the first time around? Was the post perhaps ignored until it received enough upvotes to be considered worth reading? Are some users&#8217; upvotes just trusted more than others&#8217;?)</p>

  <p>For another view of the upvote dynamics, here&#8217;s a static visualization, where we can again easily see the clusters of activity:</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/social-network-transmission/upvote-clusters-labeled-v2.png"><img src="http://dl.dropbox.com/u/10506/blog/social-network-transmission/upvote-clusters-labeled-v2.png" alt="Upvote Temporal Clusters" /></a></p>

  <h1>Fin</h1>

  <p>There are still many questions it would be interesting to look at; for example,</p>

  <ul>
  <li>What differentiates users who sparked spikes of activity from users who didn&#8217;t? I don&#8217;t believe it&#8217;s simply number of followers, as many well-connected upvoters did <em>not</em> lead to cascades of shares. Does authority matter?</li>
  <li>How far can a post reach? Clearly, the post reached people more than one degree of separation away from me (where one degree of separation is a follower); what does the distribution of degrees look like? Is there any relationship between degree of separation and time of upvote?</li>
  <li>What can we say about the people who started following me after reading my answer? Are they fewer degrees of separation away? Are they more interested in machine learning? Have they upvoted any of my answers before? (Perhaps there&#8217;s a certain &#8220;threshold&#8221; of interestingness people need to overflow before they&#8217;re considered acceptable followees.)</li>
  </ul>


  <p>But to summarize a bit what we&#8217;ve seen so far, here are some statistics on the role the social graph played in spreading the post:</p>

  <ul>
  <li>There are 5 clusters of activity after the initial post, sparked both by power users and less-connected folks. In an interesting cascade of information, some of these sparks led to further spikes in activity as well (as when Aditya Sengupta&#8217;s upvote found its way to Marc Bodnick, who set off even more activity).</li>
  <li>35% of users made their way to my answer because of someone else&#8217;s upvote.</li>
  <li>Through these connections, the post reached a fair variety of users: 32% of upvoters don&#8217;t even follow any of the post&#8217;s topics.</li>
  <li>77% of upvotes came from users over two weeks <em>after</em> my answer appeared.</li>
  <li>If we look only at the upvoters who follow at least one of the post&#8217;s topics, 33% didn&#8217;t see my answer until someone else showed it to them. In other words, a full one-third of people who presumably would have been interested in my post anyways only found it because of their social network.</li>
  </ul>


  <p>So it looks like the social graph played quite a large part in the post&#8217;s propagation, and I&#8217;ll end with a big shoutout to Stormy Shippy, who provided an awesome set of scripts I used to collect a lot of this data.</p>
  </div>  

                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/2011/08/22/introduction-to-latent-dirichlet-allocation/">Introduction to Latent Dirichlet Allocation</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/edwin-chen.html">Edwin Chen</a>
        </li>
        <li class="published" title="2011-08-22T04:15:00">
          on&nbsp;Mon 22 August 2011
        </li>

	</ul>

</div><!-- /.post-info -->
  <div class="entry-content"><h1>Introduction</h1>

  <p>Suppose you have the following set of sentences:</p>

  <ul>
  <li>I like to eat broccoli and bananas.</li>
  <li>I ate a banana and spinach smoothie for breakfast.</li>
  <li>Chinchillas and kittens are cute.</li>
  <li>My sister adopted a kitten yesterday.</li>
  <li>Look at this cute hamster munching on a piece of broccoli.</li>
  </ul>


  <p>What is latent Dirichlet allocation? It&#8217;s a way of automatically discovering <strong>topics</strong> that these sentences contain. For example, given these sentences and asked for 2 topics, LDA might produce something like</p>

  <ul>
  <li><strong>Sentences 1 and 2</strong>: 100% Topic A</li>
  <li><strong>Sentences 3 and 4</strong>: 100% Topic B</li>
  <li><strong>Sentence 5</strong>: 60% Topic A, 40% Topic B</li>
  <li><strong>Topic A</strong>: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, &#8230; (at which point, you could interpret topic A to be about food)</li>
  <li><strong>Topic B</strong>: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, &#8230; (at which point, you could interpret topic B to be about cute animals)</li>
  </ul>


  <p>The question, of course, is: how does LDA perform this discovery?</p>

  <h1>LDA Model</h1>

  <p>In more detail, LDA represents documents as <strong>mixtures of topics</strong> that spit out words with certain probabilities. It assumes that documents are produced in the following fashion: when writing each document, you</p>

  <ul>
  <li>Decide on the number of words N the document will have (say, according to a Poisson distribution).</li>
  <li>Choose a topic mixture for the document (according to a Dirichlet distribution over a fixed set of K topics). For example, assuming that we have the two food and cute animal topics above, you might choose the document to consist of 1/3 food and 2/3 cute animals.</li>
  <li>Generate each word w_i in the document by:

  <ul>
  <li>First picking a topic (according to the multinomial distribution that you sampled above; for example, you might pick the food topic with 1/3 probability and the cute animals topic with 2/3 probability).</li>
  <li>Using the topic to generate the word itself (according to the topic&#8217;s multinomial distribution). For example, if we selected the food topic, we might generate the word &#8220;broccoli&#8221; with 30% probability, &#8220;bananas&#8221; with 15% probability, and so on.</li>
  </ul>
  </li>
  </ul>


  <p>Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection.</p>

  <h2>Example</h2>

  <p>Let&#8217;s make an example. According to the above process, when generating some particular document D, you might</p>

  <ul>
  <li>Pick 5 to be the number of words in D.</li>
  <li>Decide that D will be 1/2 about food and 1/2 about cute animals.</li>
  <li>Pick the first word to come from the food topic, which then gives you the word &#8220;broccoli&#8221;.</li>
  <li>Pick the second word to come from the cute animals topic, which gives you &#8220;panda&#8221;.</li>
  <li>Pick the third word to come from the cute animals topic, giving you &#8220;adorable&#8221;.</li>
  <li>Pick the fourth word to come from the food topic, giving you &#8220;cherries&#8221;.</li>
  <li>Pick the fifth word to come from the food topic, giving you &#8220;eating&#8221;.</li>
  </ul>


  <p>So the document generated under the LDA model will be &#8220;broccoli panda adorable cherries eating&#8221; (note that LDA is a bag-of-words model).</p>

  <h1>Learning</h1>

  <p>So now suppose you have a set of documents. You&#8217;ve chosen some fixed number of K topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic. How do you do this? One way (known as collapsed Gibbs sampling) is the following:</p>

  <ul>
  <li>Go through each document, and randomly assign each word in the document to one of the K topics.</li>
  <li>Notice that this random assignment already gives you both topic representations of all the documents and word distributions of all the topics (albeit not very good ones).</li>
  <li>So to improve on them, for each document d&#8230;

  <ul>
  <li>Go through each word w in d&#8230;

  <ul>
  <li>And for each topic t, compute two things: 1) p(topic t | document d) = the proportion of words in document d that are currently assigned to topic t, and 2) p(word w | topic t) = the proportion of assignments to topic t over all documents that come from this word w. Reassign w a new topic, where we choose topic t with probability p(topic t | document d) * p(word w | topic t) (according to our generative model, this is essentially the probability that topic t generated word w, so it makes sense that we resample the current word&#8217;s topic with this probability). (Also, I&#8217;m glossing over a couple of things here, in particular the use of priors/pseudocounts in these probabilities.)</li>
  <li>In other words, in this step, we&#8217;re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.</li>
  </ul>
  </li>
  </ul>
  </li>
  <li>After repeating the previous step a large number of times, you&#8217;ll eventually reach a roughly steady state where your assignments are pretty good. So use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall).</li>
  </ul>


  <h1>Layman&#8217;s Explanation</h1>

  <p>In case the discussion above was a little eye-glazing, here&#8217;s another way to look at LDA in a different domain.</p>

  <p>Suppose you&#8217;ve just moved to a new city. You&#8217;re a hipster and an anime fan, so you want to know where the other hipsters and anime geeks tend to hang out. Of course, as a hipster, you know you can&#8217;t just <em>ask</em>, so what do you do?</p>

  <p>Here&#8217;s the scenario: you scope out a bunch of different establishments (<strong>documents</strong>) across town, making note of the people (<strong>words</strong>) hanging out in each of them (e.g., Alice hangs out at the mall and at the park, Bob hangs out at the movie theater and the park, and so on). Crucially, you don&#8217;t know the typical interest groups (<strong>topics</strong>) of each establishment, nor do you know the different interests of each person.</p>

  <p>So you pick some number K of categories to learn (i.e., you want to learn the K most important kinds of categories people fall into), and start by making a guess as to why you see people where you do. For example, you initially guess that Alice is at the mall because people with interests in X like to hang out there; when you see her at the park, you guess it&#8217;s because her friends with interests in Y like to hang out there; when you see Bob at the movie theater, you randomly guess it&#8217;s because the Z people in this city really like to watch movies; and so on.</p>

  <p>Of course, your random guesses are very likely to be incorrect (they&#8217;re random guesses, after all!), so you want to improve on them. One way of doing so is to:</p>

  <ul>
  <li>Pick a place and a person (e.g., Alice at the mall).</li>
  <li>Why is Alice likely to be at the mall? Probably because other people at the mall with the same interests sent her a message telling her to come.</li>
  <li>In other words, the more people with interests in X there are at the mall and the stronger Alice is associated with interest X (at all the other places she goes to), the more likely it is that Alice is at the mall because of interest X.</li>
  <li>So make a new guess as to why Alice is at the mall, choosing an interest with some probability according to how likely you think it is.</li>
  </ul>


  <p>Go through each place and person over and over again. Your guesses keep getting better and better (after all, if you notice that lots of geeks hang out at the bookstore, and you suspect that Alice is pretty geeky herself, then it&#8217;s a good bet that Alice is at the bookstore because her geek friends told her to go there; and now that you have a better idea of why Alice is probably at the bookstore, you can use this knowledge in turn to improve your guesses as to why everyone else is where they are), and eventually you can stop updating. Then take a snapshot (or multiple snapshots) of your guesses, and use it to get all the information you want:</p>

  <ul>
  <li>For each category, you can count the people assigned to that category to figure out what people have this particular interest. By looking at the people themselves, you can interpret the category as well (e.g., if category X contains lots of tall people wearing jerseys and carrying around basketballs, you might interpret X as the &#8220;basketball players&#8221; group).</li>
  <li>For each place P and interest category C, you can compute the proportions of people at P because of C (under the current set of assignments), and these give you a representation of P. For example, you might learn that the people who hang out at Barnes &amp; Noble consist of 10% hipsters, 50% anime fans, 10% jocks, and 30% college students.</li>
  </ul>


  <h1>Real-World Example</h1>

  <p>Finally, I applied LDA to a set of Sarah Palin&#8217;s emails a little while ago (see <a href="http://blog.echen.me/2011/06/27/topic-modeling-the-sarah-palin-emails/">here</a> for the blog post, or <a href="http://sarah-palin.heroku.com/">here</a> for an app that allows you to browse through the emails by the LDA-learned categories), so let&#8217;s give a brief recap. Here are some of the topics that the algorithm learned:</p>

  <ul>
  <li><strong>Trig/Family/Inspiration</strong>: family, web, mail, god, son, from, congratulations, children, life, child, down, trig, baby, birth, love, you, syndrome, very, special, bless, old, husband, years, thank, best, &#8230;</li>
  <li><strong>Wildlife/BP Corrosion</strong>: game, fish, moose, wildlife, hunting, bears, polar, bear, subsistence, management, area, board, hunt, wolves, control, department, year, use, wolf, habitat, hunters, caribou, program, denby, fishing, &#8230;</li>
  <li><strong>Energy/Fuel/Oil/Mining:</strong> energy, fuel, costs, oil, alaskans, prices, cost, nome, now, high, being, home, public, power, mine, crisis, price, resource, need, community, fairbanks, rebate, use, mining, villages, &#8230;</li>
  <li><strong>Gas</strong>: gas, oil, pipeline, agia, project, natural, north, producers, companies, tax, company, energy, development, slope, production, resources, line, gasline, transcanada, said, billion, plan, administration, million, industry, &#8230;</li>
  <li><strong>Education/Waste</strong>: school, waste, education, students, schools, million, read, email, market, policy, student, year, high, news, states, program, first, report, business, management, bulletin, information, reports, 2008, quarter, &#8230;</li>
  <li><strong>Presidential Campaign/Elections</strong>: mail, web, from, thank, you, box, mccain, sarah, very, good, great, john, hope, president, sincerely, wasilla, work, keep, make, add, family, republican, support, doing, p.o, &#8230;</li>
  </ul>


  <p>Here&#8217;s an example of an email which fell 99% into the Trig/Family/Inspiration category (particularly representative words are highlighted in blue):</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/palin-browser/trig-email.png"><img src="http://dl.dropbox.com/u/10506/blog/palin-browser/trig-email.png" alt="Trig Email" /></a></p>

  <p>And here&#8217;s an excerpt from an email which fell 10% into the Presidential Campaign/Election category (in red) and 90% into the Wildlife/BP Corrosion category (in green):</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/palin-browser/wildlife-presidency-email.png"><img src="http://dl.dropbox.com/u/10506/blog/palin-browser/wildlife-presidency-email.png" alt="Wildlife-Presidency Email" /></a></p>
  </div>  

                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/2011/07/28/tweets-vs-likes-what-gets-shared-on-twitter-vs-facebook/">Tweets vs. Likes: What gets shared on Twitter vs. Facebook?</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/edwin-chen.html">Edwin Chen</a>
        </li>
        <li class="published" title="2011-07-28T04:15:00">
          on&nbsp;Thu 28 July 2011
        </li>

	</ul>

</div><!-- /.post-info -->
  <div class="entry-content"><p>It always strikes me as curious that some posts get a lot of love on Twitter, while others get many more shares on Facebook:</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/twitter-beats-fb.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/twitter-beats-fb.png" alt="Twitter Beats FB" /></a></p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/fb-beats-twitter.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/fb-beats-twitter.png" alt="FB Beats Twitter" /></a></p>

  <p>What accounts for this difference? Some of it is surely site-dependent: maybe one blogger has a Facebook page but not a Twitter account, while another has these roles reversed. But even on sites maintained by a single author, tweet-to-likes ratios can vary widely from post to post.</p>

  <p>So what kinds of articles tend to be more popular on Twitter, and which spread more easily on Facebook? To take a stab at an answer, I scraped data from a couple of websites over the weekend.</p>

  <p><strong>tl;dr</strong> Twitter is still for the <em>techies</em>: articles where the number of tweets greatly outnumber FB likes tend to revolve around software companies and programming. Facebook, on the other hand, appeals to <em>everyone else</em>: yeah, to the masses, and to non-software technical folks in general as well.</p>

  <h1>FlowingData</h1>

  <p>The first site I looked at was Nathan Yau&#8217;s awesome <a href="http://www.flowingdata.com">FlowingData</a> website on data visualization. To see which articles are more popular on Facebook and which are more popular on Twitter, let&#8217;s sort all the FlowingData articles by their # tweets / # likes ratio.</p>

  <p>Here are the 10 posts with the lowest tweets-to-likes ratio (i.e., the posts that were especially popular with Facebook users):</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-facebook2.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-facebook2-small.png" alt="FlowingData Facebook" /></a></p>

  <ul>
  <li><a href="http://flowingdata.com/2011/01/30/what-your-state-is-the-worst-at-united-states-of-shame/">What your state is the worst at  United States of shame</a></li>
  <li><a href="http://flowingdata.com/2011/05/13/plush-statistical-distribution-pillows/">Plush statistical distribution pillows</a></li>
  <li><a href="http://flowingdata.com/2011/03/22/are-gas-prices-really-that-high/">Are gas prices really that high?</a></li>
  <li><a href="http://flowingdata.com/2011/01/21/hey-jude-flowchart/">Hey Jude flowchart</a></li>
  <li><a href="http://flowingdata.com/2011/04/28/womens-dress-sizes-demystified/">Womens dress sizes demystified</a></li>
  <li><a href="http://flowingdata.com/2011/02/22/america-is-not-the-best-at-everything/">America is not the best at everything</a></li>
  <li><a href="http://flowingdata.com/2011/06/10/what-you-need-to-get-together/">What you need to get together</a></li>
  <li><a href="http://flowingdata.com/2011/01/27/dexters-victims-through-season-five/">Dexters victims through season five</a></li>
  <li><a href="http://flowingdata.com/2011/05/06/correlating-dog/">Correlating dog</a></li>
  <li><a href="http://flowingdata.com/2011/02/14/valentines-day-importance/">Valentines Day importance</a></li>
  </ul>


  <p>And here are the 10 posts with the highest tweets-to-like ratio (i.e., the posts especially popular with Twitter users):</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-twitter.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-twitter-small.png" alt="FlowingData Twitter" /></a></p>

  <ul>
  <li><a href="http://flowingdata.com/2011/01/04/delicious-mass-exodus/">Delicious mass exodus</a></li>
  <li><a href="http://flowingdata.com/2011/05/25/pew-research-raw-survey-data-now-available/">Pew Research raw survey data now available</a></li>
  <li><a href="http://flowingdata.com/2011/02/03/stock-market-predictions-with-twitter/">Stock market predictions with Twitter</a></li>
  <li><a href="http://flowingdata.com/2011/01/25/growth-and-usage-of-foursquare-in-2010/">Growth and usage of foursquare in 2010</a></li>
  <li><a href="http://flowingdata.com/2011/02/17/sunlight-labs-opens-up-real-time-congress-api/">Sunlight Labs opens up Real Time Congress API</a></li>
  <li><a href="http://flowingdata.com/2011/03/25/open-source-data-science-toolkit/">Open-source Data Science Toolkit</a></li>
  <li><a href="http://flowingdata.com/2011/01/24/explore-your-linkedin-network-visually-with-inmaps/">Explore your LinkedIn network visually with InMaps</a></li>
  <li><a href="http://flowingdata.com/2011/05/03/perceived-vs-actual-country-rankings/">Perceived vs. actual country rankings</a></li>
  <li><a href="http://flowingdata.com/2011/04/20/see-what-you-and-others-tweet-about-with-the-topic-explorer/">See what you and others tweet about with the Topic Explorer</a></li>
  <li><a href="http://flowingdata.com/2011/04/24/history-of-detainees-at-guantnamo/">History of detainees at Guantnamo</a></li>
  </ul>


  <p>Notice any differences between the two?</p>

  <ul>
  <li>Instant gratification infographics, cuteness, comics, and pop culture get liked on Facebook.</li>
  <li>APIs, datasets, visualizations related to techie sites (Delicious, foursquare, Twitter, LinkedIn), and picture-less articles get tweeted instead.</li>
  </ul>


  <p>Interestingly, it also looks like the colors in the top 10 Facebook articles tend to the red end of the spectrum, while the colors in the top 10 Twitter articles tend to the blue end of the spectrum. Does this pattern hold if we look at more data? Here&#8217;s a meta-visualization of the FlowingData articles, sorted by articles popular on Facebook in the top left to articles popular on Twitter in the bottom right (see <a href="http://flowingdata-melted.heroku.com/">here</a> for some interactivity and more details):</p>

  <p><a href="http://flowingdata-melted.heroku.com/"><img src="http://dl.dropbox.com/u/10506/blog/flowingdata-metaviz/flowingdata-metaviz.png" alt="FlowingData MetaViz" /></a></p>

  <p>It does indeed look like the images at the top (the articles popular on Facebook) are more pink, while the images at the bottom (the articles popular on Twitter) are more blue (though it would be nice to quantify this in some way)!</p>

  <p>Furthermore, we can easily see from the grid that articles with no visualizations (represented by lorem ipsum text in the grid) cluster at the bottom. Grabbing some actual numbers, we find that 32% of articles with at least one picture have more shares on Facebook than on Twitter, compared to only 4% of articles with no picture at all.</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-viz-effect.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-viz-effect.png" alt="Effect of a visualization" /></a></p>

  <p>Finally, let&#8217;s break down the percentage of articles with more Facebook shares by category.</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-categories.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-categories.png" alt="FlowingData Categories" /></a></p>

  <p>(I filtered the categories so that each category in the plot above contains at least 5 articles.)</p>

  <p>What do we find?</p>

  <ul>
  <li>Articles in the Software, Online Applications, News, and Data sources categories (yawn) get 100% of their shares from Twitter.</li>
  <li>Articles tagged with <a href="http://flowingdata.com/category/projects/data-underload/">Data Underload</a> (which seems to contain short and sweet visualizations of everyday things), <a href="http://flowingdata.com/category/miscellaneous-data/">Miscellaneous</a> (which contains lots of comics or comic-like visualizations), and <a href="http://flowingdata.com/category/visualization/infographics/">Infographics</a> get the most shares on Facebook.</li>
  <li>This category breakdown matches precisely what we saw in the top 10 examples above.</li>
  </ul>


  <h1>New Scientist</h1>

  <p>When looking at FlowingData, we saw that Twitter users are much bigger on sharing technical articles. But is this true for technical articles in general, or only for programming-related posts? (In my experience with Twitter, I haven&#8217;t seen many people from math and the non-computer sciences.)</p>

  <p>To answer, I took articles from the <a href="http://www.newscientist.com/search?rbsection1=Physics+%26+Math&amp;sortby=rbpubdate">Physics &amp; Math</a> and <a href="http://www.newscientist.com/search?rbsection1=tech&amp;sortby=rbpubdate">Technology</a> sections of <a href="http://www.newscientist.com">New Scientist</a>, and</p>

  <ul>
  <li>Calculated the percentage of shares each article received on Twitter (i.e., # tweets / (# tweets + # likes)).</li>
  <li>Grouped articles by their number of tweets rounded to the nearest multiple of 25 (bin #1 contains articles close to 25 tweets, bin #2 contains articles close to 50 tweets, etc.).</li>
  <li>Calculated the median percentage of shares on Twitter for each bin.</li>
  </ul>


  <p>Here&#8217;s a graph of the result:</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/tech_vs_physicsmath.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/tech_vs_physicsmath.png" alt="Technology vs. Physics &amp; Math" /></a></p>

  <p>Notice that:</p>

  <ul>
  <li>The technology articles get consistently more shares from Twitter than the physics and math articles do.</li>
  <li>Twitter accounts for the majority of the technology shares.</li>
  <li>Facebook accounts for the majority of the physics and math shares.</li>
  </ul>


  <p>So this suggests that Twitter really is for computer technology in particular, not technical matters in general (though it would be nice to look at areas other than physics and math as well).</p>

  <h1>Quora</h1>

  <p>To get some additional evidence on the computer science vs. math/physics divide, I</p>

  <ul>
  <li>Scraped about 350 profiles of followers from each of the Computer Science, Software Engineering, Mathematics, and Physics categories on Quora;</li>
  <li>Checked each user to see whether they link to their Facebook and Twitter accounts on their profile.</li>
  </ul>


  <p>Here&#8217;s the ratio of the number of people linking to their Facebook account to the number of people linking to their Twitter account, sliced by topic:</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/math-physics-vs-cs-software.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/math-physics-vs-cs-software.png" alt="Math/Physics vs. CS/Software" /></a></p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/math-physics-vs-cs-software-collapsed.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/math-physics-vs-cs-software-collapsed.png" alt="Math/Physics vs. CS/Software, Collapsed" /></a></p>

  <p>We find exactly what we expect from the New Scientist data: people following the math and physics categories have noticeably smaller Twitter / Facebook ratios compared to people following the computer science and software engineering categories (i.e., compared to computer scientists and software engineers, mathematicians and physicists are more likely to be on Facebook than on Twitter). What&#8217;s more, this difference is in fact significant: the graphs display individual 90% confidence intervals (which overlap not at all or only slightly), and we do indeed get significance at the 95% level if we look at the differences between categories.</p>

  <p>This corroborates the New Scientist evidence that Twitter gets the computer technology shares, while Facebook gets the math and physics shares.</p>

  <h1>XKCD</h1>

  <p>Finally, let&#8217;s take a look at which XKCD comics are especially popular on Facebook vs. Twitter.</p>

  <p>Here are the 10 comics with the highest likes-to-tweets ratio (i.e., the comics especially popular on Facebook):</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/xkcd-facebook.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/xkcd-facebook-small.png" alt="XKCD Facebook" /></a></p>

  <ul>
  <li><a href="http://xkcd.com/846/">Dental Nerve</a></li>
  <li><a href="http://xkcd.com/861/">Wisdom Teeth</a></li>
  <li><a href="http://xkcd.com/876/">Trapped</a></li>
  <li><a href="http://xkcd.com/849/">Complex Conjugate</a></li>
  <li><a href="http://xkcd.com/854/">Learning to Cook</a></li>
  <li><a href="http://xkcd.com/840/">Serious</a></li>
  <li><a href="http://xkcd.com/839/">Explorers</a></li>
  <li><a href="http://xkcd.com/815/">Mu</a></li>
  <li><a href="http://xkcd.com/809/">Los Alamos</a></li>
  <li><a href="http://xkcd.com/911/">Magic School Bus</a></li>
  </ul>


  <p>Here are the 10 comics with the highest tweets-to-likes ratio (i.e., the comics especially popular on Twitter):</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/xkcd-twitter.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/xkcd-twitter-small.png" alt="XKCD Twitter" /></a></p>

  <ul>
  <li><a href="http://xkcd.com/869/">Server Attention Span</a></li>
  <li><a href="http://xkcd.com/818/">Illness</a></li>
  <li><a href="http://xkcd.com/865/">Nanobots</a></li>
  <li><a href="http://xkcd.com/912/">Manual Override</a></li>
  <li><a href="http://xkcd.com/908/">The Cloud</a></li>
  <li><a href="http://xkcd.com/810/">Constructive</a></li>
  <li><a href="http://xkcd.com/887/">Future Timeline</a></li>
  <li><a href="http://xkcd.com/844/">Good Code</a></li>
  <li><a href="http://xkcd.com/801/">Golden Hammer</a></li>
  <li><a href="http://xkcd.com/906/">Advertising Discovery</a></li>
  <li><a href="http://xkcd.com/802/">Online Communities 2</a></li>
  </ul>


  <p>Note that the XKCD comics popular on Facebook have more of a layman flavor, while the XKCD comics popular on Twitter are much more programming-related:</p>

  <ul>
  <li>Of the XKCD comics popular on Twitter, one&#8217;s about server attention spans, another&#8217;s about IPv6 addresses, a third is about GNU info pages, another deals with cloud computing, a fifth talks about Java, and the last is about a bunch of techie sites. (This is just like what we saw with the FlowingData visualizations.)</li>
  <li>Facebook, on the other hand, gets Ke$ha and Magic School Bus.</li>
  <li>And while both top 10&#8217;s contain a flowchart, the one popular on FB is about <em>cooking</em>, while the one popular on Twitter is about <em>code</em>!</li>
  <li>What&#8217;s more, if we look at the few technical-ish comics that are more popular on Facebook (the complex conjugate, mu, and Los Alamos comics), we see that they&#8217;re about physics and math, not programming (which matches our findings from the New Scientist articles).</li>
  </ul>


  <h1>Lesson</h1>

  <p>So why should you care? Here&#8217;s one takeaway:</p>

  <ul>
  <li>If you&#8217;re blogging about technology, programming, and computer science, Twitter is your friend.</li>
  <li>But if you&#8217;re blogging about anything else, be it math/physics or pop culture, don&#8217;t rely on a Twitter account alone; your shares are more likely to propagate on Facebook, so make sure to have a Facebook page as well.</li>
  </ul>


  <h1>What&#8217;s Next?</h1>

  <p>The three websites I looked at are all fairly tech-oriented, so it would be nice to gather data from other kinds of websites as well.</p>

  <p>And now that we have an idea how Twitter and Facebook compare, the next burning question is surely: <a href="http://finalbossform.com/post/7214184180/google-is-fast-becoming-the-leading-social">what do people share on Google+?!</a></p>

  <h1>Addendum</h1>

  <p>Let&#8217;s consider the following thought experiment. Suppose you come across the most unpopular article ever written. What will its FB vs. Twitter shares look like? Although no <em>real</em> person will ever share this article, I think Twitter has many more spambots (who tweet out any and every link) than FB does, so maybe unpopular articles will have more tweets than likes by default. Conversely, suppose you come across the most popular article ever written, which everybody wants to share. Then since FB has many more users than Twitter does, maybe popular articles will tend to have more likes than tweets anyways.</p>

  <p>Thus, in order to find out which types of articles are <em>especially</em> popular on FB vs. Twitter, instead of looking at tweets-to-likes ratios directly, we could try to remove this baseline popularity effect. (Taking ratios instead of raw number of tweets or raw number of likes is one kind of normalization; this is another.)</p>

  <p>So does this scenario (or something similar to it) actually play out in practice?</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-overall-popularity-vs-fb.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-overall-popularity-vs-fb.png" alt="Overall Popularity vs. Facebook" /></a></p>

  <p>Here I&#8217;ve plotted the overall popularity of a post (the total number of shares it received on either Twitter or FB) against the percentage of shares on Facebook alone, and we can see that as a post&#8217;s popularity grows, more and more shares do indeed tend to come from Facebook rather than Twitter.</p>

  <p>Also, see the posts at the lower end of the popularity scale that are only getting shares on Twitter? Let&#8217;s take a look at the five most unpopular of these:</p>

  <ul>
  <li><a href="http://flowingdata.com/2011/03/31/flowingdata-is-brought-to-you-by-8/">Flowing Data is brought to you by&#8230; (March 2011 edition)</a> (11 tweets, 0 likes)</li>
  <li><a href="http://flowingdata.com/2011/07/05/flowingdata-is-brought-to-you-by-11/">Flowing Data is brought to you by&#8230; (July 2011 edition)</a> (14 tweets, 0 likes)</li>
  <li><a href="http://flowingdata.com/2011/06/06/flowingdata-is-brought-to-you-by-10/">Flowing Data is brought to you by&#8230; (June 2011 edition)</a> (17 tweets, 0 likes)</li>
  <li><a href="http://flowingdata.com/2011/05/09/flowingdata-is-brought-to-you-by-9/">Flowing Data is brought to you by&#8230; (May 2011 edition)</a> (18 tweets, 0 likes)</li>
  <li><a href="http://flowingdata.com/2011/02/28/flowingdata-is-brought-to-you-by-7/">Flowing Data is brought to you by&#8230; (May 2011 edition)</a> (12 tweets, 1 like)</li>
  </ul>


  <p>Notice that they&#8217;re all shoutouts to FlowingData&#8217;s sponsors! There&#8217;s pretty much no reason any <em>real</em> person would share these on Twitter or Facebook, and indeed, checking Twitter to see who actually tweeted out these links, we see that the tweeters are bots:</p>

  <ul>
  <li><a href="https://twitter.com/#!/myVisualization/status/77685824224894976">https://twitter.com/#!/myVisualization/status/77685824224894976</a></li>
  <li><a href="https://twitter.com/#!/InfographicTwts/status/67668615142457344">https://twitter.com/#!/InfographicTwts/status/6766861514245734</a></li>
  <li><a href="https://twitter.com/#!/guysgoogle/status/77644902510493696">https://twitter.com/#!/guysgoogle/status/77644902510493696</a></li>
  <li><a href="https://twitter.com/#!/WhereIsYourData/status/77631743292735488">https://twitter.com/#!/WhereIsYourData/status/77631743292735488</a></li>
  </ul>


  <p>Now let&#8217;s switch to a slightly different view of the above scenario, where I plot number of tweets against number of likes:</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-tweets-vs-likes.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-tweets-vs-likes.png" alt="FlowingData Tweets vs. Likes" /></a></p>

  <p>We see that as popularity on Twitter increases, so too does popularity on Facebook &#8211; but at a slightly faster rate. (The form of the blue line plotted is roughly $\log(likes) = -3.87 + 1.70 \log(tweets)$.)</p>

  <p>So instead of looking at the ratios above, to figure out which articles are popular on FB vs. Twitter, we could look at the residuals of the above plot. Posts with large positive residuals would be posts that are especially popular on FB, and posts with negative residuals would be posts that are especially popular on Twitter.</p>

  <p>In practice, however, there wasn&#8217;t much difference between looking at residuals vs. ratios directly when using the datasets I had, so to keep things simple in the main discussion above, I stuck to ratios alone. Still, it&#8217;s another option which might be useful when looking at different questions or different sources of data, so just for completeness, here&#8217;s what the FlowingData results look like if we use residuals instead.</p>

  <p>The 10 articles with the highest residuals (i.e., the articles most popular on Facebook):</p>

  <ul>
  <li><a href="http://flowingdata.com/2011/06/10/what-you-need-to-get-together/">What you need to get together</a></li>
  <li><a href="http://flowingdata.com/2011/02/14/valentines-day-importance/">Valentines Day importance</a></li>
  <li><a href="http://flowingdata.com/2011/01/30/what-your-state-is-the-worst-at-united-states-of-shame/">What your state is the worst at  United States of shame</a></li>
  <li><a href="http://flowingdata.com/2011/05/13/plush-statistical-distribution-pillows/">Plush statistical distribution pillows</a></li>
  <li><a href="http://flowingdata.com/2011/07/01/hitler-learns-topology/">Hitler learns topology</a></li>
  <li><a href="http://flowingdata.com/2011/01/27/dexters-victims-through-season-five/">Dexters victims through season five</a></li>
  <li><a href="http://flowingdata.com/2011/07/06/access-to-education-where-you-live/">Access to education where you live</a></li>
  <li><a href="http://flowingdata.com/2011/03/09/watching-costco-warehouses-open-nationwide/">Watching the growth of Costco warehouses</a></li>
  <li><a href="http://flowingdata.com/2011/03/22/are-gas-prices-really-that-high/">Are gas prices really that high?</a></li>
  <li><a href="http://flowingdata.com/2011/01/21/flight-safety-esque-beer-pong-guide/">Flight safety-esque beer pong guide</a></li>
  </ul>


  <p>The 10 articles with the lowest residuals (i.e., the articles most popular on Twitter):</p>

  <ul>
  <li><a href="http://flowingdata.com/2011/05/25/pew-research-raw-survey-data-now-available/">Pew Research raw survey data now available</a></li>
  <li><a href="http://flowingdata.com/2011/01/24/explore-your-linkedin-network-visually-with-inmaps/">Explore your LinkedIn network visually with InMaps</a></li>
  <li><a href="http://flowingdata.com/2011/02/03/stock-market-predictions-with-twitter/">Stock market predictions with Twitter</a></li>
  <li><a href="http://flowingdata.com/2011/01/04/delicious-mass-exodus/">Delicious mass exodus</a></li>
  <li><a href="http://flowingdata.com/2011/03/25/open-source-data-science-toolkit/">Open-source Data Science Toolkit</a></li>
  <li><a href="http://flowingdata.com/2011/04/17/business-intelligence-vs-infotainment/">Business intelligence vs. infotainment</a></li>
  <li><a href="http://flowingdata.com/2011/04/20/see-what-you-and-others-tweet-about-with-the-topic-explorer/">See what you and others tweet about with the Topic Explorer</a></li>
  <li><a href="http://flowingdata.com/2011/01/25/growth-and-usage-of-foursquare-in-2010/">Growth and usage of foursquare in 2010</a></li>
  <li><a href="http://flowingdata.com/2011/05/10/flash-vs-html5/">Flash vs. HTML5</a></li>
  <li><a href="http://flowingdata.com/2011/06/09/gender-and-time-comparisons-on-twitter/">Gender and time comparisons on Twitter</a></li>
  </ul>


  <p>Here&#8217;s a density plot of article residuals, split by whether the article has a visualization or not (residuals of picture-free articles are clearly shifted towards the negative end):</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/has-viz-residuals.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/has-viz-residuals.png" alt="Residuals" /></a></p>

  <p>Here are the mean residuals per category (again, we see that the miscellaneous, data underload, data art, and infographics categories tend to be more popular on Facebook, while the data sources, software, online applications, and news categories tend to be more popular on Twitter):</p>

  <p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/category-residuals.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/category-residuals.png" alt="Category Residuals" /></a></p>

  <p>And that&#8217;s it! In the spirit of these findings, I hope this article gets <a href="http://blog.echen.me/2011/07/28/tweets-vs-likes-what-gets-shared-on-twitter-vs-facebook/?share=facebook&amp;nb=1">liked</a> a little and <a href="https://twitter.com/share?original_referer=http%3A%2F%2Fblog.echen.me%2F2011%2F07%2F28%2Ftweets-vs-likes-what-gets-shared-on-twitter-vs-facebook%2F&amp;source=tweetbutton&amp;text=Tweets%20vs.%20Likes%3A%20What%20gets%20shared%20on%20Twitter%20vs.%20Facebook%3F%3A&amp;url=http%3A%2F%2Fwp.me%2Fpy9AS-6P">tweeted</a> lots and lots.</p>
  </div>  
<script type= "text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>

                    </article>
 
<div class="paginator">
            <div class="navButton"><a href="/index2.html">Prev</a></div>
    <div class="navButton">Page 3 / 7</div>
        <div class="navButton"><a href="/index4.html" >Next</a></div>
</div>
                </div>
            </aside><!-- /#featured -->
            
        
  
	      <div class="LaunchyardDetail">
	        <p>
	          <a class="title" href="/">Edwin Chen</a>
	          <br/>
	          Hanging. MIT, Microsoft Research, Clarium, Twitter, Google.
	          <br /><br />
            <a href="mailto:hello[at]echen.me">Email</a><br />
            <a href="https://twitter.com/#!/echen">Twitter</a><br/>
            <a href="https://github.com/echen">Github</a><br/>
            <a href="https://plus.google.com/113804726252165471503/">Google+</a><br/>
            <a href="http://www.linkedin.com/in/edwinchen1">LinkedIn</a><br/>
            <a href="http://quora.com/edwin-chen-1">Quora</a>
          </p>
          <br />
          <div id="recent_posts">
              <h3>Recent Posts</h3>
                <a href="/2014/08/15/propensity-modeling-causal-inference-and-discovering-drivers-of-growth/">Propensity Modeling, Causal Inference, and Discovering Drivers of Growth  </a><br /><br />
                <a href="/2013/01/08/improving-twitter-search-with-real-time-human-computation/">Improving Twitter Search with Real-Time Human Computation  </a><br /><br />
                <a href="/2012/07/31/edge-prediction-in-a-social-graph-my-solution-to-facebooks-user-recommendation-contest-on-kaggle/">Edge Prediction in a Social Graph: My Solution to Facebook's User Recommendation Contest on Kaggle  </a><br /><br />
                <a href="/2012/07/06/soda-vs-pop-with-twitter/">Soda vs. Pop with Twitter  </a><br /><br />
                <a href="/2012/04/25/making-the-most-of-mechanical-turk-tips-and-best-practices/">Making the Most of Mechanical Turk: Tips and Best Practices  </a><br /><br />
                <a href="/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Infinite Mixture Models with Nonparametric Bayes and the Dirichlet Process  </a><br /><br />
                <a href="/2012/03/05/instant-interactive-visualization-with-d3-and-ggplot2/">Instant Interactive Visualization with d3 + ggplot2  </a><br /><br />
                <a href="/2012/02/09/movie-recommendations-and-more-via-mapreduce-and-scalding/">Movie Recommendations and More via MapReduce and Scalding  </a><br /><br />
                <a href="/2012/01/17/quick-introduction-to-ggplot2/">Quick Introduction to ggplot2  </a><br /><br />
                <a href="/2012/01/03/introduction-to-conditional-random-fields/">Introduction to Conditional Random Fields  </a><br /><br />
            
          </div>
        </div>


        <section id="extras" >
       
        
        </section><!-- /#extras -->
	
        <footer id="contentinfo" >
                <address id="about" class="vcard ">
                Proudly powered by <a href="http://getpelican.com/" target="_blank">Pelican</a>, which takes
                great advantage of <a href="http://python.org" target="_blank">Python</a>.
		
                </address><!-- /#about -->
		

                
        </footer><!-- /#contentinfo -->

</body>
</html>