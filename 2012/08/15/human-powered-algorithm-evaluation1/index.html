<!DOCTYPE html>
<html lang="en">
<head>
        <title>Human-Powered Algorithm Evaluation1</title>
        <meta charset="utf-8" />
	      <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie.css"/>
                <script src="/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie6.css"/><![endif]-->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js" type="text/javascript"></script>


</head>

<body id="index" class="home">
	
<section id="content" >
    <div class="body">
      <article>
        <header>
          <h1 class="entry-title">
            <a href="/2012/08/15/human-powered-algorithm-evaluation1/" rel="bookmark"
               title="Permalink to Human-Powered Algorithm Evaluation1">Human-Powered Algorithm Evaluation1</a></h1>

        </header>

        <div class="entry-content">
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/edwin-chen.html">Edwin Chen</a>
        </li>
        <li class="published" title="2012-08-15T00:00:00">
          on&nbsp;Wed 15 August 2012
        </li>

	</ul>

</div><!-- /.post-info -->          <p>Discovery algorithms are one of the keystones of many online sites. How often has YouTube's related videos algorithm led you into a sinkhole of funny cats? How many books have you purchased through Amazon's <a href="http://www.quora.com/How-does-Amazons-collaborative-filtering-recommendation-engine-work/answer/Edwin-Chen-1"><em>Customers Who Bought This Item Also Bought</em> </a> feature?</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/yt-related-videos.png"><img alt="YouTube Related Videos" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/yt-related-videos.png" /></a>
<br/>
<a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/amazon-related-books.png"><img alt="Amazon Related Books" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/amazon-related-books.png" /></a>
<br /></p>
<p>So how can we measure their performance, and understand where they could be improved?</p>
<p>I'm a big fan of <a href="http://blog.echen.me/2013/01/08/improving-twitter-search-with-real-time-human-computation/"><em>man-in-the-machine</em> approaches</a>  and <a href="http://blog.echen.me/2012/04/25/making-the-most-of-mechanical-turk-tips-and-best-practices/">crowdsourcing in general</a>, so I'm going to talk about a <strong>human evaluation</strong> approach to measuring the quality of <em>Do This Next</em> algorithms and other discovery products.</p>
<h1>Beyond Log-Based Metrics</h1>
<p>Let's first motivate why standard, live experiment metrics aren't always sufficient and why we need humans at all.</p>
<p>So imagine you suddenly replace all of Facebook's ads with pornography. What's going to happen? Well, CTR's likely to shoot through the roof!</p>
<p>Or, imagine instead that you replace all of Amazon's related books with shinier, more expensive editions. Again, CTR and revenue are likely to increase, as the flashier content draws eyeballs. But is this anything but a short-term boost? Perhaps the change decreases total sales in the long-term, as customers start to find Amazon too expensive and gaudy for their tastes and move to other marketplaces instead.</p>
<p>How do we guard against scenarios like these? Well, this is a related books algorithm after all! So why, by sticking to a live experiment and standard metrics like CTR and revenue, are we nowhere measuring the relatedness of our recommendations?</p>
<h1>Human Evaluation</h1>
<p>The answer, of course, is that computers can't measure relevance or relatedness very well; if they could, we'd be done. So let's inject humans into the process.</p>
<p>In the screenshot below, I asked a worker on Mechanical Turk to rate the first three related book suggestions for a book on <a href="http://www.barnesandnoble.com/">barnesandnoble.com</a>.</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/fools-assassin-rating.png"><img alt="Fool's Assassin" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/fools-assassin-rating.png" /></a></p>
<p>The recommendations seem to be decent, but could be much better, and we see a couple ways to improve them:</p>
<ul>
<li>First, two of the recommendations (<em>The Broken Eye</em> and <em>Tower Lord</em>) are weird, since they're each part of a series, but not the first book in the series. So one improvement would be to ensure that non-first-in-series books don't get displayed unless they're followups to the main book.</li>
<li>Book covers matter! Indeed, the second suggestion looks more like a fantasy romance novel than the kind of fantasy that Robin Hobb tends to write. (So perhaps B&amp;N should start investing in some deep learning...)</li>
</ul>
<p>In general, then, here's one way to understand the baseline quality of a <em>Do This Next</em> algorithm:</p>
<ol>
<li>Take a bunch of items (e.g., books if we're Amazon or Barnes &amp; Noble), and generate their related brethren.</li>
<li>Send these <item, related items> pairs off to a bunch of judges (e.g., using a crowdsourcing platform like Mechanical Turk), and ask them to rate their relevance.</li>
<li>Analyze the data that comes back.</li>
</ol>
<h1>An Example: Read This Next on Amazon, Barnes &amp; Noble, and Google</h1>
<p>Let's make this concrete. Pretend I'm a newly minted VP of book suggestions at Amazon, and I want to understand the product's flaws and stars.</p>
<p>I started by going to Mechanical Turk and asking a couple hundred workers to take a book they've read and enjoyed in the past year, and search for it on Amazon. They'd then take the first three related book suggestions from a different author, rate them on the following scale*, and explain their ratings.</p>
<ul>
<li>Great suggestion. I'd definitely buy it. (Very positive)</li>
<li>Decent suggestion. I might buy it. (Mildly positive)</li>
<li>Not a great suggestion. I probably wouldn't buy it. (Mildly negative)</li>
<li>Terrible suggestion. I definitely wouldn't buy it. (Very negative)</li>
</ul>
<p>*Note: I usually prefer a three-point or five-point Likert scale (basically, adding a "neutral" option), but I decided to go wild with this task.</p>
<p>How well did Amazon do? Quite well: 47% of raters said they'd definitely buy the first related book, another 29% said it was good enough that they might buy it, and only 24% of raters disliked the first suggestion.</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/a1-ratings.png"><img alt="A1 Ratings" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/a1-ratings.png" /></a></p>
<p>The other book suggestions, while worse, seem to perform pretty well too: around 65% of raters were positive on the second and third book suggestions.</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/a2-ratings.png"><img alt="A2 Ratings" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/a2-ratings.png" /></a></p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/a3-ratings.png"><img alt="A3 Ratings" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/a3-ratings.png" /></a></p>
<p>What can we learn from the bad ratings? I ran another task that asked workers to categorize the bad related books, and here's the breakdown.</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/bad-categories.png"><img alt="Bad Suggestions Breakdown" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/bad-categories.png" /></a></p>
<ul>
<li><strong>Related but different-subtopic.</strong> These were book suggestions that were generally related to the original book, but that were in a different sub-topic that the rater wasn't interested in. For example, the first related book for <em>The Art of War</em> was <a href="http://www.amazon.com/On-War-Carl-von-Clausewitz/dp/1469947021"><em>On War</em></a>, but <em>"I would not buy this book, because it only focuses on military war. I am not interested in that. I am interested in mental tactics that will help me prosper in life."</em></li>
<li><strong>Completely unrelated.</strong> These were book suggestions that were completely unrelated to the original book. For example, a Scrabble dictionary appearing on <em>The Art of War</em>'s page.</li>
<li><strong>Uninteresting.</strong> These were suggestions that were related, but whose storylines didn't appeal to the rater. <em>"The storyline doesn't seem that exciting.  I am not a dog fan and it's about a dog."</em></li>
<li><strong>Wrong audience.</strong> These were book suggestions whose target audiences were quite different from the original book's audiences. In many cases, for example, a related book suggestion would be a children's book, but the original book would be geared towards adults. <em>"This seems to be a children's book. If I had a child I would definitely buy this; alas I do not, so I have no need for it."</em></li>
<li><strong>Wrong book type.</strong> Suggestions in this category would be textbooks or books of reviews appearing alongside novels.</li>
<li><strong>Disliked author.</strong> These recommendations would be by a similar author, but one that the rater had negative past experiences with. <em>"I do not like Amber Portwood.  I would definitely not want to read a book by and about her."</em></li>
<li><strong>Not first in series.</strong> Some book recommendations would be for an interesting series the rater wasn't familiar with, but they wouldn't be the first book in the series.</li>
<li><strong>Bad rating.</strong> These were book suggestions that had a particularly low Amazon rating.</li>
</ul>
<h2>Competitive Analysis</h2>
<h1>Personalized vs. Non-Personalized</h1>
<p>Here's one subtlety. In my example above, I asked raters to pick a starting book (one that they read and loved in the past year), and then rate whether they themselves would want to read the related suggestions.</p>
<p>Another approach is to pick the starting books <em>for</em> the raters, and then have the rate the related suggestions somewhat objectively, by trying to put themselves in the shoes of someone who'd be interested in the starting item.</p>
<p>Which approach is better? I don't think there's a clear answer -- it depends on the task and goals at hand.</p>
<p>Pros of the first approach:</p>
<ul>
<li>It's much more nuanced. It can often be hard to put yourself in someone else's shoes, after all. Would someone reading Harry Potter be interested in Twilight? On the one hand, they're both fantasy books; on the other hand, Twilight seems a bit more geared towards female audiences.</li>
</ul>
<p>Take this concrete example from YouTube (thanks to Michael Lombardo) that I really enjoy.</p>
<p>In the instance above, a rater picked a video of relaxing music for her dog. If I myself were rating each of the related video suggestions, I'd probably say that a two minute-long video of soothing music is a great related video. Both are relaxing music geared towards dogs! But the rater made a clear case why a two minute video actually isn't a great suggestion, and was the first example that really helped me understand ...</p>
<p>Pros of the second approach:</p>
<ul>
<li>Sometimes, objectivity is a good thing. (Should you really care if someone dislikes Twilight simply because it was her ex-boyfriend's favorite book?)</li>
<li>Allowing people to choose their starting items may bias our metrics. For instance, people are much more likely to choose popular books to rate, whereas we might want to measure the quality of Amazon's suggestions across a broader, more representative slice of its catalog.</li>
</ul>
<h1>Side-by-Sides</h1>
<p>So far, I've focused on describing <em>absolute</em> judgments: judges rate how relevant a book (or whatever item) is to the original on an absolute scale.</p>
<p>In many cases, though, <em>side-by-side</em> judgments, wherein judges are given two items and asked which one is <em>better</em>, are preferable, since comparative judgments are often much easier to make than absolute ones, and we might want to detect differences at a finer level than what's available on an absolute scale.</p>
<p>To switch examples temporarily, suppose we're trying to compare the quality of Google's and Bing's search algorithms, and the search engines surface the following Wikipedia page as the first result on a search for OKCupid's Christian Rudder.</p>
<p>The <em>captions</em> under both results are pretty mediocre. I'm probably searching for the OK Cupid dude, but neither result makes it absolutely clear whether this is him. So if I were rating each result in isolation, I'd probably assign them the same neutral score.</p>
<p>But if I were comparing them, </p>
<h1>Recap</h1>
<p>Best Practices
- multiple judgments
- gold standard, plastic
- ask for comments
- clear instructions</p>
<h1>Sid</h1>
<ul>
<li>
<p>personalized vs. non-personalized</p>
</li>
<li>
<p>use cases</p>
</li>
<li>metrics</li>
<li>how much to improve</li>
<li></li>
<li>other companies</li>
</ul>
<h1>Side-by-Sides</h1>
<h1>Other benefits</h1>
<ul>
<li>quicker iteration</li>
</ul>
<p>*Side question:</p>
<h1>Quora</h1>
<h1>Launch Decisions</h1>
<h1>Amazon Homepage</h1>
<h1>Search</h1>
<h1>Twitter Ads</h1>
        </div><!-- /.entry-content -->

      </article>
    </div>
</section>
  
	      <div class="LaunchyardDetail">
	        <p>
	          <a class="title" href="/">Edwin Chen</a>
	          <br/>
	          Hanging. MIT, MSR, Clarium, Twitter, Google, Dropbox.
	          <br /><br />
            <a href="mailto:hello[at]echen.me">Email</a><br />
            <a href="https://twitter.com/#!/echen">Twitter</a><br/>
            <a href="https://github.com/echen">Github</a><br/>
            <a href="https://plus.google.com/113804726252165471503/">Google+</a><br/>
            <a href="http://www.linkedin.com/in/edwinchen1">LinkedIn</a><br/>
            <a href="http://quora.com/edwin-chen-1">Quora</a>
          </p>
          <br />
          <div id="recent_posts">
              <h3>Recent Posts</h3>
                <a href="/2014/08/15/human-powered-algorithm-evaluation/">Human-Powered Algorithm Evaluation  </a><br /><br />
                <a href="/2014/08/15/propensity-modeling-causal-inference-and-discovering-drivers-of-growth/">Propensity Modeling, Causal Inference, and Discovering Drivers of Growth  </a><br /><br />
                <a href="/2013/01/08/improving-twitter-search-with-real-time-human-computation/">Improving Twitter Search with Real-Time Human Computation  </a><br /><br />
                <a href="/2012/08/15/human-powered-algorithm-evaluation1/">Human-Powered Algorithm Evaluation1  </a><br /><br />
                <a href="/2012/08/15/human-powered-algorithm-evaluation2/">Human-Powered Algorithm Evaluation2  </a><br /><br />
                <a href="/2012/07/31/edge-prediction-in-a-social-graph-my-solution-to-facebooks-user-recommendation-contest-on-kaggle/">Edge Prediction in a Social Graph: My Solution to Facebook's User Recommendation Contest on Kaggle  </a><br /><br />
                <a href="/2012/07/06/soda-vs-pop-with-twitter/">Soda vs. Pop with Twitter  </a><br /><br />
                <a href="/2012/04/25/making-the-most-of-mechanical-turk-tips-and-best-practices/">Making the Most of Mechanical Turk: Tips and Best Practices  </a><br /><br />
                <a href="/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Infinite Mixture Models with Nonparametric Bayes and the Dirichlet Process  </a><br /><br />
                <a href="/2012/03/05/instant-interactive-visualization-with-d3-and-ggplot2/">Instant Interactive Visualization with d3 + ggplot2  </a><br /><br />
            
          </div>
        </div>


        <section id="extras" >
       
        
        </section><!-- /#extras -->
	
        <footer id="contentinfo" >
                <address id="about" class="vcard ">
                Proudly powered by <a href="http://getpelican.com/" target="_blank">Pelican</a>, which takes
                great advantage of <a href="http://python.org" target="_blank">Python</a>.
		
                </address><!-- /#about -->
		

                
        </footer><!-- /#contentinfo -->

</body>
</html>