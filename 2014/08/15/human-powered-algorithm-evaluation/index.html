<!DOCTYPE html>
<html lang="en">
<head>
        <title>Human-Powered Algorithm Evaluation</title>
        <meta charset="utf-8" />
	      <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie.css"/>
                <script src="/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie6.css"/><![endif]-->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js" type="text/javascript"></script>


</head>

<body id="index" class="home">
	
<section id="content" >
    <div class="body">
      <article>
        <header>
          <h1 class="entry-title">
            <a href="/2014/08/15/human-powered-algorithm-evaluation/" rel="bookmark"
               title="Permalink to Human-Powered Algorithm Evaluation">Human-Powered Algorithm Evaluation</a></h1>

        </header>

        <div class="entry-content">
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/edwin-chen.html">Edwin Chen</a>
        </li>
        <li class="published" title="2014-08-15T00:00:00">
          on&nbsp;Fri 15 August 2014
        </li>

	</ul>

</div><!-- /.post-info -->          <p>Item-item similarities are a cornerstone of many online sites. How often has YouTube's related videos algorithm drawn you into a sinkhole of funny cats? How many books have you purchased through Amazon's <a href="http://www.quora.com/How-does-Amazons-collaborative-filtering-recommendation-engine-work/answer/Edwin-Chen-1"><em>Customers Who Bought This Item Also Bought</em> </a> feature?</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/yt-related-videos.png"><img alt="YouTube Related Videos" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/yt-related-videos.png" /></a>
<br/>
<a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/amazon-related-books.png"><img alt="Amazon Related Books" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/amazon-related-books.png" /></a>
<br /></p>
<p>So how can we measure their performance, and understand where they could be improved?</p>
<p>I'm a big fan of <a href="http://blog.echen.me/2013/01/08/improving-twitter-search-with-real-time-human-computation/"><em>man-in-the-machine</em> techniques</a>, and <a href="http://blog.echen.me/2012/04/25/making-the-most-of-mechanical-turk-tips-and-best-practices/">crowdsourcing in general</a>, so I'm going to talk about a <strong>human evaluation</strong> approach to measuring the quality of <em>Do This Next</em> algorithms and other discovery products.</p>
<h1>Beyond Log-Based Metrics</h1>
<p>Let's first motivate why log-based quality metrics aren't always sufficient and why we need humans at all.</p>
<p>Take Amazon's related books feature. To measure its effectiveness, the standard approach is to run a live experiment and measure the change in metrics like overall revenue or CTR.</p>
<p>But imagine we suddenly replace all of Amazon's book suggestions with pornography. What's going to happen? <em>CTR's likely to shoot through the roof!</em></p>
<p>Or suppose that we replace all of Amazon's related books with shinier, more expensive items. Again, CTR and revenue are likely to increase, as the flashier content draws eyeballs in the short term. But is this anything but a short-term boost? Perhaps the change decreases total sales in the long-term, as customers start to find Amazon too expensive and gaudy for their tastes and move to other marketplaces instead.</p>
<p>How can we help avoid scenarios like these? Well, this is a <em>related</em> books algorithm after all. So why, by sticking to a live experiment and metrics like CTR, are we nowhere measuring the relatedness of our recommendations?</p>
<h1>Human Evaluation</h1>
<p>Solution: let's inject humans into the process.</p>
<p>In the screenshot below, for example, I asked a worker on Mechanical Turk to rate the first three related book suggestions for a book on <a href="http://www.barnesandnoble.com/">barnesandnoble.com</a>.</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/fools-assassin-rating.png"><img alt="Fool's Assassin" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/fools-assassin-rating.png" /></a></p>
<p>The recommendations seem to be decent, but already we see a couple ways to improve them:</p>
<ul>
<li>First, two of the recommendations (<em>The Broken Eye</em> and <em>Tower Lord</em>) are weird, since they're each part of a series, but not the first book in the series. So one improvement would be to ensure that non-first-in-series books don't get displayed unless they're followups to the main book.</li>
<li>Book covers matter! Indeed, the second suggestion looks more like a fantasy romance novel than the kind of fantasy that Robin Hobb tends to write. (So perhaps B&amp;N should invest in some deep learning...)</li>
</ul>
<p>CTR and revenue certainly wouldn't give us this level of information. They obviously couldn't give us this level of insight, and it's not clear that could tell us that our algorithms are even producing irrelevant suggestions in the first place:</p>
<ul>
<li>Nowhere does the related scroll panel make it clear that the two non-first-in-series books are part of a series, so the CTR on those two books would be just as high as if they were indeed the first in the series.</li>
<li>If revenue is low (or high), it's not clear whether it's because the suggestions are bad or because our pricing algorithms need improvement.</li>
</ul>
<p>In general, then, here's one way to understand the baseline quality of a <em>Do This Next</em> algorithm:</p>
<ol>
<li>Take a bunch of items (e.g., books if we're Amazon or Barnes &amp; Noble), and generate their related brethren.</li>
<li>Send these <item, related items> pairs off to a bunch of judges (e.g., using a crowdsourcing platform like Mechanical Turk), and ask them to rate their relevance.</li>
<li>Analyze the data that comes back.</li>
</ol>
<h1>Algorithmic Relevance on Amazon, Barnes &amp; Noble, and Google</h1>
<p>Let's make this concrete. Pretend I'm a newly minted VP of What Customers Also Buy at Amazon, and I want to understand the product's flaws and stars.</p>
<p>I started by going to Mechanical Turk and asking a couple hundred workers to take a book they read and enjoyed in the past year, and to search for it on Amazon. They'd then take the first three related book suggestions from a different author, rate them on the following scale*, and explain their ratings.</p>
<ul>
<li>Great suggestion. I'd definitely buy it. (Very positive)</li>
<li>Decent suggestion. I might buy it. (Mildly positive)</li>
<li>Not a great suggestion. I probably wouldn't buy it. (Mildly negative)</li>
<li>Terrible suggestion. I definitely wouldn't buy it. (Very negative)</li>
</ul>
<p>*Note: I usually prefer a three-point or five-point Likert scale (basically, adding a "neutral" option), but I was feeling a little wild with this task.</p>
<p>For example:</p>
<p>So how well did Amazon do? Quite well: 47% of raters said they'd definitely buy the first related book, another 29% said it was good enough that they might buy it, and only 24% of raters disliked the first suggestion.</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/a1-ratings.png"><img alt="A1 Ratings" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/a1-ratings.png" /></a></p>
<p>The other book suggestions, while a bit worse, seem to perform pretty well too: around 65% of raters were positive on the second and third <em>Customers Who Bought This Item Also Bought</em> suggestions.</p>
<p>What can we learn from the bad ratings? I ran another task that asked workers to categorize the bad related books, and here's the breakdown.</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/bad-categories.png"><img alt="Bad Suggestions Breakdown" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/bad-categories.png" /></a></p>
<ul>
<li><strong>Related but different-subtopic.</strong> These were book suggestions that were generally related to the original book, but that were in a different sub-topic that the rater wasn't interested in. For example, the first related book for Sun Tzu's <em>The Art of War</em> (a book nominally about war, but which nowadays has become more of a life hack book) was <a href="http://www.amazon.com/On-War-Carl-von-Clausewitz/dp/1469947021"><em>On War</em></a>, but the rater was only interested in the self-help aspects: <em>"I would not buy this book, because it only focuses on military war. I am not interested in that. I am interested in mental tactics that will help me prosper in life."</em></li>
<li><strong>Completely unrelated.</strong> These were book suggestions that were completely unrelated to the original book. For example, a Scrabble dictionary appearing on <em>The Art of War</em>'s page.</li>
<li><strong>Uninteresting.</strong> These were suggestions that were related, but whose storylines didn't appeal to the rater. <em>"The storyline doesn't seem that exciting.  I am not a dog fan and it's about a dog."</em></li>
<li><strong>Wrong audience.</strong> These were book suggestions whose target audiences were quite different from the original book's audiences. In many cases, for example, a related book suggestion would be a children's book, but the original book would be geared towards adults. <em>"This seems to be a children's book. If I had a child I would definitely buy this; alas I do not, so I have no need for it."</em></li>
<li><strong>Wrong book type.</strong> Suggestions in this category were items like textbooks or books of reviews appearing alongside novels.</li>
<li><strong>Disliked author.</strong> These recommendations would be by a similar author, but one that the rater disliked. <em>"I do not like Amber Portwood.  I would definitely not want to read a book by and about her."</em></li>
<li><strong>Not first in series.</strong> Some book recommendations would be for an interesting series the rater wasn't familiar with, but they wouldn't be the first book in the series.</li>
<li><strong>Bad rating.</strong> These were book suggestions that had a particularly low Amazon rating.</li>
</ul>
<p>So to improve their recommendations, Amazon could try improving its topic models, adding age-based features to its books, distinguishing between textbooks and novels, and investing in series detectors. (Of course, for all I know they do some of this already. Even with the right features, making good recommendations is still an extremely difficult problem.)</p>
<h2>Competitive Analysis</h2>
<p>Now that we have a general grasp of Amazon's related book suggestions, one question would be: how does it compare to other online booksellers like Barnes &amp; Noble and the Google Play Bookstore?</p>
<p>I took the same task I used for Amazon, but this time also asked raters to review the first three related suggestions (from a different author) on those two sites as well.</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/amazon-bn-google.png"><img alt="Amazon vs. B&amp;N vs. Google" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/amazon-bn-google.png" /></a></p>
<p>In short: Barnes &amp; Nobles's algorithm are almost as good as Amazon's (the first three suggestions were rated positive 68% of the time on Amazon, compared to 58% of the time on Barnes &amp; Noble), but the Play Store's recommendations are atrocious: a whopping 51% of Google's related book recommendations were rated as a terrible suggestion.</p>
<p>Why are the Play Store's so bad? Let's take a look at a couple examples.</p>
<p>Here's the Play Store page for John Green's <em>The Fault in Our Stars</em>, a well-reviewed book about cancer and romance (now also turned into a movie).</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/fault-in-our-stars.png"><img alt="Fault in Our Stars" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/fault-in-our-stars.png" /></a></p>
<p>Four of the suggestions are from different John Green's writing in totally separate genres, and two of the suggestions are randomly a poorly-viewed Excel manual and a poorly-reviewed textbook on sexual health.</p>
<p>Here's another page, for Guillermo del Toro's <em>The Strain</em>.
<a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/the-strain.png"><img alt="The Strain" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/the-strain.png" /></a></p>
<p>In this case, all the suggestions are in a different language, and there are only four of them.</p>
<p>So once again asking raters to categorize all of the Play Store's bad recommendations, here's a breakdown.</p>
<p><a href="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/google-atrocity.png"><img alt="Google Atrocity" src="https://dl.dropboxusercontent.com/u/10506/blog/human-evaluation/google-atrocity.png" /></a></p>
<ul>
<li>45% of the time, the related book suggestions were completely unrelated to the original book in question. For example: displaying a physics textbook on the page for a romance novel.</li>
<li>32% of the time, there simply wasn't any book suggestion by a different author at all. (I'm guessing the Play Bookstore's catalog is pretty small, but even a random recommendation from the same genre as the original book would probably be better than nothing.)</li>
<li>14% of the time, the related books were in a different language!</li>
</ul>
<p>So despite Google's state-of-the-art machine learning elsewhere, its Play Store suggestions couldn't really get much worse.</p>
<h1>Side-by-Sides</h1>
<p>Let's step back a bit. So far I've been focusing on an <em>absolute</em> judgments paradigm, in which judges rate how relevant a book is to the original on an absolute scale. This model is great for understanding the overall, baseline quality of Amazon's related book algorithm.</p>
<p>In many cases, though, we want to use human evaluation to <em>compare</em> experiments. For example, it's common at many companies to:</p>
<ol>
<li>Launch a "human evaluation A/B test" before a live experiment, both to avoid accidentally sending out incredibly buggy experiments to users, as well as to avoid the long wait required in live experiments.</li>
<li>Use a human-generated relevance score as a supplement to live experiment metrics when making launch decisions.</li>
</ol>
<p>For these kinds of tasks, what's preferable is often a <em>side-by-side</em> model, wherein judges are given two items and asked which one is better. After all, comparative judgments are often much easier to make than absolute ones, and we might want to detect differences at a finer level than what's available on an absolute scale.</p>
<p>Unfortunately, I don't have an easy way to generate data for a side-by-side (though I could perform a side-by-side on Amazon vs. Barnes &amp; Noble), so I'll omit an example, but the idea should be pretty clear.</p>
<h1>Personalization</h1>
<p>Here's another subtlety. In my examples above, I asked raters to pick a starting book themselves (one that they read and loved in the past year), and then rate whether they personally would want to read the related suggestions.</p>
<p>Another approach is to pick the starting books <em>for</em> the raters, and then have the rate the related suggestions more objectively, by trying to put themselves in the shoes of someone who'd be interested in the starting item.</p>
<p>Which approach is better? As you can probably guess, there's no clear answer – it depends on the task and goals at hand.</p>
<p>Pros of the first approach:</p>
<ul>
<li>It's much more nuanced. It can often be hard to put yourself in someone else's shoes, after all. Would someone reading Harry Potter be interested in Twilight? On the one hand, they're both fantasy books; on the other hand, Twilight seems a bit more geared towards female audiences.</li>
</ul>
<p>Take this concrete example from YouTube (thanks to Michael Lombardo) that I really enjoy.</p>
<p>In the instance above, a rater picked a video of relaxing music for her dog. If I myself were rating each of the related video suggestions, I'd probably say that a two minute-long video of soothing music is a great related video. Both are relaxing music geared towards dogs! But the rater made a clear case why a two minute video actually isn't a great suggestion, and was the first example that really helped me understand ...</p>
<p>Pros of the second approach:</p>
<ul>
<li>Sometimes, objectivity is a good thing. (Should you really care if someone dislikes Twilight simply because it was her ex-boyfriend's favorite book?)</li>
<li>Allowing people to choose their starting items may bias certain metrics. For instance, people are much more likely to choose popular books to rate, whereas we might want to measure the quality of Amazon's suggestions across a broader, more representative slice of its catalog.</li>
</ul>
<h1>Recap</h1>
<p>Let's review what I've discussed so far.</p>
<p>Best Practices
- multiple judgments
- gold standard, plastic
- ask for comments
- clear instructions</p>
<h1>Sid</h1>
<ul>
<li>
<p>personalized vs. non-personalized</p>
</li>
<li>
<p>use cases</p>
</li>
<li>metrics</li>
<li>how much to improve</li>
<li></li>
<li>other companies</li>
</ul>
<h1>Side-by-Sides</h1>
<h1>Other benefits</h1>
<ul>
<li>quicker iteration</li>
</ul>
        </div><!-- /.entry-content -->

      </article>
    </div>
</section>
  
	      <div class="LaunchyardDetail">
	        <p>
	          <a class="title" href="/">Edwin Chen</a>
	          <br/>
	          Hanging. MIT, MSR, Clarium, Twitter, Google, Dropbox.
	          <br /><br />
            <a href="mailto:hello[at]echen.me">Email</a><br />
            <a href="https://twitter.com/#!/echen">Twitter</a><br/>
            <a href="https://github.com/echen">Github</a><br/>
            <a href="https://plus.google.com/113804726252165471503/">Google+</a><br/>
            <a href="http://www.linkedin.com/in/edwinchen1">LinkedIn</a><br/>
            <a href="http://quora.com/edwin-chen-1">Quora</a>
          </p>
          <br />
          <div id="recent_posts">
              <h3>Recent Posts</h3>
                <a href="/2014/08/15/human-powered-algorithm-evaluation/">Human-Powered Algorithm Evaluation  </a><br /><br />
                <a href="/2014/08/15/propensity-modeling-causal-inference-and-discovering-drivers-of-growth/">Propensity Modeling, Causal Inference, and Discovering Drivers of Growth  </a><br /><br />
                <a href="/2013/01/08/improving-twitter-search-with-real-time-human-computation/">Improving Twitter Search with Real-Time Human Computation  </a><br /><br />
                <a href="/2012/08/15/human-powered-algorithm-evaluation1/">Human-Powered Algorithm Evaluation1  </a><br /><br />
                <a href="/2012/08/15/human-powered-algorithm-evaluation2/">Human-Powered Algorithm Evaluation2  </a><br /><br />
                <a href="/2012/07/31/edge-prediction-in-a-social-graph-my-solution-to-facebooks-user-recommendation-contest-on-kaggle/">Edge Prediction in a Social Graph: My Solution to Facebook's User Recommendation Contest on Kaggle  </a><br /><br />
                <a href="/2012/07/06/soda-vs-pop-with-twitter/">Soda vs. Pop with Twitter  </a><br /><br />
                <a href="/2012/04/25/making-the-most-of-mechanical-turk-tips-and-best-practices/">Making the Most of Mechanical Turk: Tips and Best Practices  </a><br /><br />
                <a href="/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Infinite Mixture Models with Nonparametric Bayes and the Dirichlet Process  </a><br /><br />
                <a href="/2012/03/05/instant-interactive-visualization-with-d3-and-ggplot2/">Instant Interactive Visualization with d3 + ggplot2  </a><br /><br />
            
          </div>
        </div>


        <section id="extras" >
       
        
        </section><!-- /#extras -->
	
        <footer id="contentinfo" >
                <address id="about" class="vcard ">
                Proudly powered by <a href="http://getpelican.com/" target="_blank">Pelican</a>, which takes
                great advantage of <a href="http://python.org" target="_blank">Python</a>.
		
                </address><!-- /#about -->
		

                
        </footer><!-- /#contentinfo -->

</body>
</html>