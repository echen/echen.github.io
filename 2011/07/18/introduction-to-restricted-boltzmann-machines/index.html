<!DOCTYPE html>
<html lang="en">
<head>
        <title>Introduction to Restricted Boltzmann Machines</title>
        <meta charset="utf-8" />
	      <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="stylesheet" href="../../../../theme/css/main.css" type="text/css" />
        <link href="http://blog.echen.me/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Edwin Chen's Blog Atom Feed" />
        <link href="http://blog.echen.me/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Edwin Chen's Blog RSS Feed" />

        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.4/gist-embed.min.js"></script>

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="../../../../css/ie.css"/>
                <script src="../../../../js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="../../../../css/ie6.css"/><![endif]-->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js" type="text/javascript"></script>


</head>

<body id="index" class="home">

<section id="content" >
    <div class="body">
      <article>
        <header>
          <h1 class="entry-title">
            <a href="../../../../2011/07/18/introduction-to-restricted-boltzmann-machines/" rel="bookmark"
               title="Permalink to Introduction to Restricted Boltzmann Machines">Introduction to Restricted Boltzmann Machines</a></h1>

        </header>

        <div class="entry-content">
<div class="post-info">

</div><!-- /.post-info -->          <p>Suppose you ask a bunch of users to rate a set of movies on a 0-100 scale. In classical factor analysis, you could then try to explain each movie and user in terms of a set of latent <strong>factors</strong>. For example, movies like Star Wars and Lord of the Rings might have strong associations with a latent science fiction and fantasy factor, and users who like Wall-E and Toy Story might have strong associations with a latent Pixar factor.</p>
<p>Restricted Boltzmann Machines essentially perform a <em>binary</em> version of factor analysis. (This is one way of thinking about RBMs; there are, of course, others, and lots of different ways to use RBMs, but I’ll adopt this approach for this post.) Instead of users rating a set of movies on a continuous scale, they simply tell you whether they like a movie or not, and the RBM will try to discover latent factors that can explain the activation of these movie choices.</p>
<p>More technically, a Restricted Boltzmann Machine is a <strong>stochastic neural network</strong> (neural network meaning we have neuron-like units whose binary activations depend on the neighbors they’re connected to; stochastic meaning these activations have a probabilistic element) consisting of:</p>
<ul>
<li>One layer of <strong>visible units</strong> (users’ movie preferences whose states we know and set);</li>
<li>One layer of <strong>hidden units</strong> (the latent factors we try to learn); and</li>
<li>A bias unit (whose state is always on, and is a way of adjusting for the different inherent popularities of each movie).</li>
</ul>
<p>Furthermore, each visible unit is connected to all the hidden units (this connection is undirected, so each hidden unit is also connected to all the visible units), and the bias unit is connected to all the visible units and all the hidden units. To make learning easier, we restrict the network so that no visible unit is connected to any other visible unit and no hidden unit is connected to any other hidden unit.</p>
<p>For example, suppose we have a set of six movies (Harry Potter, Avatar, LOTR 3, Gladiator, Titanic, and Glitter) and we ask users to tell us which ones they want to watch. If we want to learn two latent units underlying movie preferences – for example, two natural groups in our set of six movies appear to be SF/fantasy (containing Harry Potter, Avatar, and LOTR 3) and Oscar winners (containing LOTR 3, Gladiator, and Titanic), so we might hope that our latent units will correspond to these categories – then our RBM would look like the following:</p>
<p><img alt="RBM Example" src="http://i.imgur.com/sadnLks.png"></p>
<p>(Note the resemblance to a factor analysis graphical model.)</p>
<h1>State Activation</h1>
<p>Restricted Boltzmann Machines, and neural networks in general, work by updating the states of some neurons given the states of others, so let’s talk about how the states of individual units change. Assuming we know the connection weights in our RBM (we’ll explain how to learn these below), to update the state of unit <span class="math">\(i\)</span>:</p>
<ul>
<li>Compute the <strong>activation energy</strong> <span class="math">\(a_i = \sum_j w_{ij} x_j\)</span> of unit <span class="math">\(i\)</span>, where the sum runs over all units <span class="math">\(j\)</span> that unit <span class="math">\(i\)</span> is connected to, <span class="math">\(w_{ij}\)</span> is the weight of the connection between <span class="math">\(i\)</span> and <span class="math">\(j\)</span>, and <span class="math">\(x_j\)</span> is the 0 or 1 state of unit <span class="math">\(j\)</span>. In other words, all of unit <span class="math">\(i\)</span>’s neighbors send it a message, and we compute the sum of all these messages.</li>
<li>Let <span class="math">\(p_i = \sigma(a_i)\)</span>, where <span class="math">\(\sigma(x) = 1/(1 + exp(-x))\)</span> is the logistic function. Note that <span class="math">\(p_i\)</span> is close to 1 for large positive activation energies, and <span class="math">\(p_i\)</span> is close to 0 for negative activation energies.</li>
<li>We then turn unit <span class="math">\(i\)</span> on with probability <span class="math">\(p_i\)</span>, and turn it off with probability <span class="math">\(1 - p_i\)</span>.</li>
<li>(In layman’s terms, units that are positively connected to each other try to get each other to share the same state (i.e., be both on or off), while units that are negatively connected to each other are enemies that prefer to be in different states.)</li>
</ul>
<p>For example, let’s suppose our two hidden units really do correspond to SF/fantasy and Oscar winners.</p>
<ul>
<li>If Alice has told us her six binary preferences on our set of movies, we could then ask our RBM which of the hidden units her preferences activate (i.e., ask the RBM to explain her preferences in terms of latent factors). So the six movies send messages to the hidden units, telling them to update themselves. (Note that even if Alice has declared she wants to watch Harry Potter, Avatar, and LOTR 3, this doesn’t guarantee that the SF/fantasy hidden unit will turn on, but only that it will turn on with high probability. This makes a bit of sense: in the real world, Alice wanting to watch all three of those movies makes us highly suspect she likes SF/fantasy in general, but there’s a small chance she wants to watch them for other reasons. Thus, the RBM allows us to generate models of people in the messy, real world.)</li>
<li>Conversely, if we know that one person likes SF/fantasy (so that the SF/fantasy unit is on), we can then ask the RBM which of the movie units that hidden unit turns on (i.e., ask the RBM to generate a set of movie recommendations). So the hidden units send messages to the movie units, telling them to update their states. (Again, note that the SF/fantasy unit being on doesn’t guarantee that we’ll always recommend all three of Harry Potter, Avatar, and LOTR 3 because, hey, not everyone who likes science fiction liked Avatar.)</li>
</ul>
<h1>Learning Weights</h1>
<p>So how do we learn the connection weights in our network? Suppose we have a bunch of training examples, where each training example is a binary vector with six elements corresponding to a user’s movie preferences. Then for each epoch, do the following:</p>
<ul>
<li>Take a training example (a set of six movie preferences). Set the states of the visible units to these preferences.</li>
<li>Next, update the states of the hidden units using the logistic activation rule described above: for the <span class="math">\(j\)</span>th hidden unit, compute its activation energy <span class="math">\(a_j = \sum_i w_{ij} x_i\)</span>, and set <span class="math">\(x_j\)</span> to 1 with probability <span class="math">\(\sigma(a_j)\)</span> and to 0 with probability <span class="math">\(1 - \sigma(a_j)\)</span>. Then for each edge <span class="math">\(e_{ij}\)</span>, compute <span class="math">\(Positive(e_{ij}) = x_i * x_j\)</span> (i.e., for each pair of units, measure whether they’re both on).</li>
<li>Now <strong>reconstruct</strong> the visible units in a similar manner: for each visible unit, compute its activation energy <span class="math">\(a_i\)</span>, and update its state. (Note that this reconstruction may not match the original preferences.) Then update the hidden units again, and compute <span class="math">\(Negative(e_{ij}) = x_i * x_j\)</span> for each edge.</li>
<li>Update the weight of each edge <span class="math">\(e_{ij}\)</span> by setting <span class="math">\(w_{ij} = w_{ij} + L * (Positive(e_{ij}) - Negative(e_{ij}))\)</span>, where <span class="math">\(L\)</span> is a learning rate.</li>
<li>Repeat over all training examples.</li>
</ul>
<p>Continue until the network converges (i.e., the error between the training examples and their reconstructions falls below some threshold) or we reach some maximum number of epochs.</p>
<p>Why does this update rule make sense? Note that</p>
<ul>
<li>In the first phase, <span class="math">\(Positive(e_{ij})\)</span> measures the association between the <span class="math">\(i\)</span>th and <span class="math">\(j\)</span>th unit that we want the network to learn from our training examples;</li>
<li>In the “reconstruction” phase, where the RBM generates the states of visible units based on its hypotheses about the hidden units alone, <span class="math">\(Negative(e_{ij})\)</span> measures the association that the network itself generates (or “daydreams” about) when no units are fixed to training data.</li>
</ul>
<p>So by adding <span class="math">\(Positive(e_{ij}) - Negative(e_{ij})\)</span> to each edge weight, we’re helping the network’s daydreams better match the reality of our training examples.</p>
<p>(You may hear this update rule called <strong>contrastive divergence</strong>, which is basically a fancy term for “approximate gradient descent”.)</p>
<h1>Examples</h1>
<p>I wrote a <a href="https://github.com/echen/restricted-boltzmann-machines">simple RBM implementation</a> in Python (the code is heavily commented, so take a look if you’re still a little fuzzy on how everything works), so let’s use it to walk through some examples.</p>
<p>First, I trained the RBM using some fake data.</p>
<ul>
<li>Alice: (Harry Potter = 1, Avatar = 1, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). Big SF/fantasy fan.</li>
<li>Bob: (Harry Potter = 1, Avatar = 0, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). SF/fantasy fan, but doesn’t like Avatar.</li>
<li>Carol: (Harry Potter = 1, Avatar = 1, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). Big SF/fantasy fan.</li>
<li>David: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Big Oscar winners fan.</li>
<li>Eric: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Oscar winners fan, except for Titanic.</li>
<li>Fred: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Big Oscar winners fan.</li>
</ul>
<p>The network learned the following weights:</p>
<p><img alt="Weights" src="http://i.imgur.com/mbsjJmX.png"></p>
<p>Note that the first hidden unit seems to correspond to the Oscar winners, and the second hidden unit seems to correspond to the SF/fantasy movies, just as we were hoping.</p>
<p>What happens if we give the RBM a new user, George, who has (Harry Potter = 0, Avatar = 0, LOTR 3 = 0, Gladiator = 1, Titanic = 1, Glitter = 0) as his preferences? It turns the Oscar winners unit on (but not the SF/fantasy unit), correctly guessing that George probably likes movies that are Oscar winners.</p>
<p>What happens if we activate only the SF/fantasy unit, and run the RBM a bunch of different times? In my trials, it turned on Harry Potter, Avatar, and LOTR 3 three times; it turned on Avatar and LOTR 3, but not Harry Potter, once; and it turned on Harry Potter and LOTR 3, but not Avatar, twice. Note that, based on our training examples, these generated preferences do indeed match what we might expect real SF/fantasy fans want to watch.</p>
<h1>Modifications</h1>
<p>I tried to keep the connection-learning algorithm I described above pretty simple, so here are some modifications that often appear in practice:</p>
<ul>
<li>Above, <span class="math">\(Negative(e_{ij})\)</span> was determined by taking the product of the <span class="math">\(i\)</span>th and <span class="math">\(j\)</span>th units after reconstructing the visible units once and then updating the hidden units again. We could also take the product after some larger number of reconstructions (i.e., repeat updating the visible units, then the hidden units, then the visible units again, and so on); this is slower, but describes the network’s daydreams more accurately.</li>
<li>Instead of using <span class="math">\(Positive(e_{ij})=x_i * x_j\)</span>, where <span class="math">\(x_i\)</span> and <span class="math">\(x_j\)</span> are binary 0 or 1 states, we could also let <span class="math">\(x_i\)</span> and/or <span class="math">\(x_j\)</span> be activation probabilities. Similarly for <span class="math">\(Negative(e_{ij})\)</span>.</li>
<li>We could penalize larger edge weights, in order to get a sparser or more regularized model.</li>
<li>When updating edge weights, we could use a momentum factor: we would add to each edge a weighted sum of the current step as described above (i.e., <span class="math">\(L * (Positive(e_{ij}) - Negative(e_{ij})\)</span>) and the step previously taken.</li>
<li>Instead of using only one training example in each epoch, we could use batches of examples in each epoch, and only update the network’s weights after passing through all the examples in the batch. This can speed up the learning by taking advantage of fast matrix-multiplication algorithms.</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </div><!-- /.entry-content -->

      </article>
    </div>
</section>

	      <div class="LaunchyardDetail">
	        <p>
	          <a style="text-align: left" class="title" href="../../../../">Edwin Chen</a>
	          <br/>

	        </p>

	        <div id="about" style="text-align: left !important">
	          <p>
	            Founder at <a href="https://www.surgehq.ai">Surge AI</a>, the world's most powerful data labeling platform and workforce for NLP.
            </p>
            <br />
            <p>
              Need obsessively high-quality human-powered data? Reach out!</a> We help top AI companies like OpenAI, Amazon, and Airbnb create stunning high-skill, human-labeled datasets.
	          </p>
            <br />
            <p>
              Former AI & engineering lead at Google, Facebook, Twitter, Dropbox, and MSR. Pure math, theoretical CS, and linguistics at MIT.
            </p>
            <br />
	          <div style="text-align: center">
              <a href="https://www.surgehq.ai">Surge AI</a><br />
              <a href="https://www.twitter.com/@HelloSurgeAI">Surge AI Twitter</a><br />
              <a href="https://surge-ai.medium.com">Surge AI Blog</a><br />
              <a href="https://github.com/surge-ai">Surge AI Github</a><br />
              <a href="https://www.linkedin.com/company/surge-ai">Surge AI LinkedIn</a><br />
              <a href="https://twitter.com/echen">Twitter</a><br/>
              <a href="https://www.linkedin.com/in/edwinzchen">LinkedIn</a><br/>                 
              <a href="https://github.com/echen">Github</a><br/>           
              <a href="https://quora.com/edwin-chen-1">Quora</a><br />
              <a href="mailto:hello[&aelig;]echen.me">Email</a><br/>
            </div>
          </div>

          <div id="recent_posts">
              <h3>Recent Posts</h3>
                <a style="font-size: 0.9em" href="../../../../2021/12/23/a-laymans-introduction-to-perplexity-in-nlp/">A Layman's Introduction to Perplexity in NLP  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2021/12/23/an-introduction-to-inter-annotator-agreement-and-cohens-kappa-statistic/">An Introduction to Inter-Annotator Agreement and Cohen's Kappa Statistic  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2021/11/21/a-visual-laymans-introduction-to-language-models-in-nlp/">A Visual, Layman's Introduction to Language Models in NLP  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2020/11/30/surge-ai-a-new-data-labeling-platform-and-workforce-for-nlp/">Surge AI: A New Data Labeling Platform and Workforce for NLP  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2017/05/30/exploring-lstms/">Exploring LSTMs  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2014/10/07/moving-beyond-ctr-better-recommendations-through-human-evaluation/">Moving Beyond CTR: Better Recommendations Through Human Evaluation  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2014/08/15/propensity-modeling-causal-inference-and-discovering-drivers-of-growth/">Propensity Modeling, Causal Inference, and Discovering Drivers of Growth  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2014/08/14/product-insights-for-airbnb/">Product Insights for Airbnb  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2013/01/08/improving-twitter-search-with-real-time-human-computation">Improving Twitter Search with Real-Time Human Computation  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2012/07/31/edge-prediction-in-a-social-graph-my-solution-to-facebooks-user-recommendation-contest-on-kaggle/">Edge Prediction in a Social Graph: My Solution to Facebook's User Recommendation Contest on Kaggle  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2012/07/06/soda-vs-pop-with-twitter/">Soda vs. Pop with Twitter  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Infinite Mixture Models with Nonparametric Bayes and the Dirichlet Process  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2012/03/05/instant-interactive-visualization-with-d3-and-ggplot2/">Instant Interactive Visualization with d3 + ggplot2  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2012/02/09/movie-recommendations-and-more-via-mapreduce-and-scalding/">Movie Recommendations and More via MapReduce and Scalding  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2012/01/17/quick-introduction-to-ggplot2/">Quick Introduction to ggplot2  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2012/01/03/introduction-to-conditional-random-fields/">Introduction to Conditional Random Fields  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/10/24/winning-the-netflix-prize-a-summary/">Winning the Netflix Prize: A Summary  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/09/29/stuff-harvard-people-like/">Stuff Harvard People Like  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/09/07/information-transmission-in-a-social-network-dissecting-the-spread-of-a-quora-post/">Information Transmission in a Social Network: Dissecting the Spread of a Quora Post  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/08/22/introduction-to-latent-dirichlet-allocation/">Introduction to Latent Dirichlet Allocation  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/07/18/introduction-to-restricted-boltzmann-machines/">Introduction to Restricted Boltzmann Machines  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/06/27/topic-modeling-the-sarah-palin-emails/">Topic Modeling the Sarah Palin Emails  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/05/01/unsupervised-language-detection-algorithms/">Filtering for English Tweets: Unsupervised Language Detection on Twitter  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/04/27/choosing-a-machine-learning-classifier/">Choosing a Machine Learning Classifier  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/04/25/kickstarter-data-analysis-success-and-pricing/">Kickstarter Data Analysis: Success and Pricing  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/04/21/a-mathematical-introduction-to-least-angle-regression/">A Mathematical Introduction to Least Angle Regression  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/04/16/introduction-to-cointegration-and-pairs-trading/">Introduction to Cointegration and Pairs Trading  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/03/14/counting-clusters/">Counting Clusters  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/03/14/hacker-news-analysis/">Hacker News Analysis  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/03/14/laymans-introduction-to-measure-theory/">Layman's Introduction to Measure Theory  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/03/14/laymans-introduction-to-random-forests/">Layman's Introduction to Random Forests  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/03/14/netflix-prize-summary-factorization-meets-the-neighborhood/">Netflix Prize Summary: Factorization Meets the Neighborhood  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/03/14/netflix-prize-summary-scalable-collaborative-filtering-with-jointly-derived-neighborhood-interpolation-weights/">Netflix Prize Summary: Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/03/14/prime-numbers-and-the-riemann-zeta-function/">Prime Numbers and the Riemann Zeta Function  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/03/14/topological-combinatorics-and-the-evasiveness-conjecture/">Topological Combinatorics and the Evasiveness Conjecture  </a><br /><br />
                <a style="font-size: 0.9em" href="../../../../2011/02/15/item-to-item-collaborative-filtering-with-amazons-recommendation-system/">Item-to-Item Collaborative Filtering with Amazon's Recommendation System  </a><br /><br />
          </div>
        </div>


        <section id="extras" >


        </section><!-- /#extras -->

        <footer id="contentinfo" >
                <address id="about" class="vcard ">
                Proudly powered by <a href="http://getpelican.com/" target="_blank">Pelican</a>, which takes
                great advantage of <a href="http://python.org" target="_blank">Python</a>.

                </address><!-- /#about -->



        </footer><!-- /#contentinfo -->

</body>
</html>